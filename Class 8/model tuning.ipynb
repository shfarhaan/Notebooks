{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835a5ea3",
   "metadata": {},
   "source": [
    "# Class 8: Model Selection, Hyperparameter Tuning, CV, and Overfitting Control\n",
    "\n",
    "**Lab:** Tune regression model for **battery life prediction**\n",
    "\n",
    "### Overview\n",
    "\n",
    "Today we turn “works on my machine” into “works on *unseen* data.” You’ll set up proper splits, run CV, tune hyperparameters with `GridSearchCV`/`RandomizedSearchCV`, read learning curves, and choose between **bias** (too simple) and **variance** (too wiggly).\n",
    "\n",
    "### Lecture Notes (pocket edition)\n",
    "\n",
    "* **Generalization**: performance on new data matters more than train score.\n",
    "* **Data splits**: **train** (fit), **validation** (tune), **test/holdout** (final check). With CV, validation is simulated by rotating folds.\n",
    "* **Cross-validation**: K-Fold (regression), **Stratified** for classification. Repeated K-Fold gives stabler estimates.\n",
    "* **Pipelines**: prevent leakage; scale/transform **inside** CV.\n",
    "* **Tuning**: Grid (systematic), Randomized (efficient on large spaces).\n",
    "* **Overfitting control**: regularization (Ridge/Lasso), early stopping (GBMs/NNs), pruning (trees), more data, simpler features.\n",
    "* **Diagnostics**: learning curves (sample size vs error), validation curves (hyperparameter vs error), residual plots.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Notebook (Colab-style)\n",
    "\n",
    "### 0) Setup: Synthetic Battery Dataset (self-contained)\n",
    "\n",
    "```python\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "np.random.seed(8)\n",
    "\n",
    "# Features that influence remaining battery capacity (Ah) or health (%)\n",
    "n = 1200\n",
    "cycles = np.random.randint(50, 1500, size=n)                # cycle count\n",
    "dod    = np.random.uniform(0.2, 0.9, size=n)                 # depth of discharge\n",
    "tempC  = np.random.normal(28, 6, size=n).clip(5, 55)         # ambient/storage temp\n",
    "crate  = np.random.uniform(0.2, 2.0, size=n)                  # charge/discharge C-rate\n",
    "rimOhm = np.random.normal(45, 10, size=n).clip(15, 120)       # internal resistance (mΩ)\n",
    "\n",
    "# Nonlinear ground truth with interactions + noise\n",
    "# capacity_% ~ 100 - a1*cycles - a2*(dod^1.3) - a3*(temp-25)^2 - a4*crate + a5*sqrt(r)\n",
    "true = (\n",
    "    100\n",
    "    - 0.02*cycles\n",
    "    - 12*(dod**1.3)\n",
    "    - 0.03*((tempC-25.0)**2)\n",
    "    - 3.0*crate\n",
    "    + 0.4*np.sqrt(rimOhm)\n",
    ")\n",
    "\n",
    "noise = np.random.normal(0, 2.5, size=n)\n",
    "capacity_pct = (true + noise).clip(0, 100)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"cycles\": cycles, \"dod\": dod, \"tempC\": tempC, \"crate\": crate, \"rimOhm\": rimOhm,\n",
    "    \"capacity_pct\": capacity_pct\n",
    "})\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 1) Train/Holdout split (hold test for the *very* end)\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=[\"capacity_pct\"])\n",
    "y = df[\"capacity_pct\"]\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train.shape, X_holdout.shape\n",
    "```\n",
    "\n",
    "### 2) Baselines + Cross-validation (Pipelines to avoid leakage)\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = \"neg_root_mean_squared_error\"  # RMSE (negative, because sklearn maximizes)\n",
    "\n",
    "pipelines = {\n",
    "    \"Linear\": Pipeline([(\"sc\", StandardScaler()), (\"lr\", LinearRegression())]),\n",
    "    \"Ridge\":  Pipeline([(\"sc\", StandardScaler()), (\"rg\", Ridge(alpha=1.0))]),\n",
    "    \"Lasso\":  Pipeline([(\"sc\", StandardScaler()), (\"ls\", Lasso(alpha=0.01, max_iter=5000))]),\n",
    "    \"RF\":     Pipeline([(\"rf\", RandomForestRegressor(n_estimators=300, random_state=42))]),\n",
    "}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    rmse_scores = -cross_val_score(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    print(f\"{name:7s} | CV RMSE: {rmse_scores.mean():.2f} ± {rmse_scores.std():.2f}\")\n",
    "```\n",
    "\n",
    "### 3) Grid Search (Ridge/Lasso) + Randomized Search (RandomForest)\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Ridge grid\n",
    "ridge_grid = {\n",
    "    \"rg__alpha\": np.logspace(-3, 3, 13)\n",
    "}\n",
    "ridge_pipe = Pipeline([(\"sc\", StandardScaler()), (\"rg\", Ridge())])\n",
    "ridge_cv = GridSearchCV(ridge_pipe, ridge_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "print(\"Ridge best:\", ridge_cv.best_params_, \"CV RMSE:\", -ridge_cv.best_score_)\n",
    "\n",
    "# Lasso grid\n",
    "lasso_grid = {\n",
    "    \"ls__alpha\": np.logspace(-4, 0, 9)\n",
    "}\n",
    "lasso_pipe = Pipeline([(\"sc\", StandardScaler()), (\"ls\", Lasso(max_iter=10000))])\n",
    "lasso_cv = GridSearchCV(lasso_pipe, lasso_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(\"Lasso best:\", lasso_cv.best_params_, \"CV RMSE:\", -lasso_cv.best_score_)\n",
    "\n",
    "# RandomForest randomized search\n",
    "rf_pipe = Pipeline([(\"rf\", RandomForestRegressor(random_state=42))])\n",
    "rf_dist = {\n",
    "    \"rf__n_estimators\": randint(200, 800),\n",
    "    \"rf__max_depth\": randint(3, 18),\n",
    "    \"rf__min_samples_split\": randint(2, 12),\n",
    "    \"rf__min_samples_leaf\": randint(1, 8),\n",
    "    \"rf__max_features\": [\"auto\", \"sqrt\", 0.5, None],\n",
    "}\n",
    "rf_cv = RandomizedSearchCV(rf_pipe, rf_dist, n_iter=30, cv=cv, scoring=scoring, n_jobs=-1, random_state=42)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "print(\"RF best:\", rf_cv.best_params_, \"CV RMSE:\", -rf_cv.best_score_)\n",
    "```\n",
    "\n",
    "### 4) Polynomial features + Ridge (bias–variance tradeoff)\n",
    "\n",
    "```python\n",
    "poly_pipe = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(include_bias=False)),   # we'll tune the degree\n",
    "    (\"sc\", StandardScaler(with_mean=False)),           # with poly, sparse-ish -> keep with_mean=False\n",
    "    (\"rg\", Ridge())\n",
    "])\n",
    "\n",
    "poly_grid = {\n",
    "    \"poly__degree\": [1, 2, 3],\n",
    "    \"rg__alpha\": np.logspace(-3, 2, 10)\n",
    "}\n",
    "\n",
    "poly_cv = GridSearchCV(poly_pipe, poly_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "poly_cv.fit(X_train, y_train)\n",
    "print(\"PolyRidge best:\", poly_cv.best_params_, \"CV RMSE:\", -poly_cv.best_score_)\n",
    "```\n",
    "\n",
    "### 5) Learning curve (do we need more data or simpler model?)\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "best_model = rf_cv.best_estimator_  # or poly_cv.best_estimator_\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 6),\n",
    "    cv=cv, scoring=scoring, n_jobs=-1, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_rmse = -train_scores.mean(axis=1)\n",
    "val_rmse   = -val_scores.mean(axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, train_rmse, marker=\"o\", label=\"Train RMSE\")\n",
    "plt.plot(train_sizes, val_rmse, marker=\"o\", label=\"CV RMSE\")\n",
    "plt.xlabel(\"Training samples\"); plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Learning Curve (best model)\")\n",
    "plt.grid(True); plt.legend(); plt.show()\n",
    "```\n",
    "\n",
    "### 6) Final evaluation on untouched holdout\n",
    "\n",
    "```python\n",
    "final_model = rf_cv.best_estimator_ if -rf_cv.best_score_ <= -poly_cv.best_score_ else poly_cv.best_estimator_\n",
    "final_model.fit(X_train, y_train)\n",
    "pred = final_model.predict(X_holdout)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "rmse = mean_squared_error(y_holdout, pred, squared=False)\n",
    "mae  = mean_absolute_error(y_holdout, pred)\n",
    "\n",
    "print(\"HOLDOUT RMSE:\", round(rmse, 2), \"| MAE:\", round(mae, 2))\n",
    "\n",
    "# Residual plot\n",
    "resid = y_holdout - pred\n",
    "plt.figure()\n",
    "plt.scatter(pred, resid, s=15, alpha=0.7)\n",
    "plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "plt.title(\"Residuals vs Predicted (Holdout)\")\n",
    "plt.xlabel(\"Predicted capacity (%)\"); plt.ylabel(\"Residual\")\n",
    "plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### 7) Model interpretation quick-peek (if RF wins)\n",
    "\n",
    "```python\n",
    "if \"rf\" in final_model.named_steps:\n",
    "    rf = final_model.named_steps[\"rf\"]\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "    print(importances)\n",
    "    importances.plot(kind=\"bar\", title=\"RF Feature Importance\"); plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Classworks (5) — Skeleton Code (students fill the blanks)\n",
    "\n",
    "### Classwork 1: Compare baselines with proper CV\n",
    "\n",
    "```python\n",
    "# ==================================================\n",
    "# CLASSWORK 1: BASELINES + CV (NO LEAKAGE!)\n",
    "# ==================================================\n",
    "# Task:\n",
    "# 1) Make a Pipeline(StandardScaler, LinearRegression) and Pipeline(StandardScaler, Ridge).\n",
    "# 2) Evaluate with KFold(n_splits=5, shuffle=True).\n",
    "# 3) Report mean±std CV RMSE for both.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# cv = KFold(n_splits=??, shuffle=True, random_state=0)\n",
    "# pipe_lr = Pipeline([(\"sc\", StandardScaler()), (\"lr\", LinearRegression())])\n",
    "# pipe_rg = Pipeline([(\"sc\", StandardScaler()), (\"rg\", Ridge(alpha=1.0))])\n",
    "\n",
    "# for name, pipe in [(\"LR\", pipe_lr), (\"Ridge\", pipe_rg)]:\n",
    "#     rmse = -cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "#     print(name, \"CV RMSE:\", rmse.mean(), \"±\", rmse.std())\n",
    "```\n",
    "\n",
    "### Classwork 2: Grid search Ridge/Lasso with param heat\n",
    "\n",
    "```python\n",
    "# ==================================================\n",
    "# CLASSWORK 2: GRIDSEARCH RIDGE/LASSO\n",
    "# ==================================================\n",
    "# Task:\n",
    "# 1) GridSearchCV over alpha (logspace).\n",
    "# 2) Print best params + CV RMSE.\n",
    "# 3) Fit on full train and evaluate on holdout.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ridge_pipe = Pipeline([(\"sc\", StandardScaler()), (\"rg\", Ridge())])\n",
    "# ridge_grid = {\"rg__alpha\": np.logspace(-3, 3, 13)}\n",
    "# ridge_cv = GridSearchCV(ridge_pipe, ridge_grid, cv=???, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "# ridge_cv.fit(X_train, y_train)\n",
    "# print(\"Ridge best:\", ridge_cv.???, \"CV RMSE:\", -ridge_cv.???)\n",
    "\n",
    "# best_ridge = ridge_cv.best_estimator_.fit(X_train, y_train)\n",
    "# hold_rmse = mean_squared_error(y_holdout, best_ridge.predict(X_holdout), squared=False)\n",
    "# print(\"Holdout RMSE:\", hold_rmse)\n",
    "```\n",
    "\n",
    "### Classwork 3: PolynomialFeatures + Ridge validation curve\n",
    "\n",
    "```python\n",
    "# ==================================================\n",
    "# CLASSWORK 3: POLY + RIDGE VALIDATION\n",
    "# ==================================================\n",
    "# Task:\n",
    "# 1) Pipeline(PolynomialFeatures -> StandardScaler -> Ridge).\n",
    "# 2) Loop degrees in {1,2,3} and alphas in logspace; store CV RMSE in a table.\n",
    "# 3) Pick the combo minimizing CV RMSE; note overfitting patterns (deg=3 vs deg=1).\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# degrees = [1,2,3]\n",
    "# alphas = np.logspace(-3, 2, 10)\n",
    "# rows = []\n",
    "# for d in degrees:\n",
    "#     for a in alphas:\n",
    "#         pipe = Pipeline([(\"poly\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "#                          (\"sc\", StandardScaler(with_mean=False)),\n",
    "#                          (\"rg\", Ridge(alpha=a))])\n",
    "#         rmse = -cross_val_score(pipe, X_train, y_train, cv=???, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n",
    "#         rows.append({\"deg\": d, \"alpha\": a, \"cv_rmse\": rmse.mean()})\n",
    "# pd.DataFrame(rows).sort_values(\"cv_rmse\").head()\n",
    "```\n",
    "\n",
    "### Classwork 4: RandomizedSearch on RandomForest + feature importance\n",
    "\n",
    "```python\n",
    "# ==================================================\n",
    "# CLASSWORK 4: RANDOMIZED RF + IMPORTANCE\n",
    "# ==================================================\n",
    "# Task:\n",
    "# 1) RandomizedSearchCV over RF hyperparameters (n_estimators, max_depth, min_samples_*).\n",
    "# 2) Report best params + CV RMSE.\n",
    "# 3) Fit best on full train, compute holdout RMSE.\n",
    "# 4) Bar plot feature importances.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=0)\n",
    "# dist = {\n",
    "#   \"n_estimators\": randint(200, 800),\n",
    "#   \"max_depth\": randint(3, 18),\n",
    "#   \"min_samples_split\": randint(2, 12),\n",
    "#   \"min_samples_leaf\": randint(1, 8),\n",
    "# }\n",
    "# rf_cv = RandomizedSearchCV(rf, dist, n_iter=30, cv=???, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, random_state=0)\n",
    "# rf_cv.fit(X_train, y_train)\n",
    "# print(\"Best:\", rf_cv.best_params_, \"CV RMSE:\", -rf_cv.best_score_)\n",
    "# best_rf = rf_cv.best_estimator_.fit(X_train, y_train)\n",
    "# print(\"Holdout RMSE:\", mean_squared_error(y_holdout, best_rf.predict(X_holdout), squared=False))\n",
    "\n",
    "# pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False).plot(kind=\"bar\")\n",
    "# plt.title(\"RF Feature Importance\"); plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### Classwork 5: Learning curve & overfitting diagnosis\n",
    "\n",
    "```python\n",
    "# ==================================================\n",
    "# CLASSWORK 5: LEARNING CURVE DIAGNOSIS\n",
    "# ==================================================\n",
    "# Task:\n",
    "# 1) Pick your best model (from CW2–CW4).\n",
    "# 2) Plot learning curve (train_sizes 10%→100%, KFold CV).\n",
    "# 3) Interpret: high bias vs high variance? Suggest a fix.\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# best_model = ???  # your best estimator\n",
    "# sizes, tr, va = learning_curve(best_model, X_train, y_train, train_sizes=np.linspace(0.1,1.0,6),\n",
    "#                                cv=???, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, shuffle=True, random_state=0)\n",
    "# plt.figure()\n",
    "# plt.plot(sizes, -tr.mean(axis=1), marker=\"o\", label=\"Train RMSE\")\n",
    "# plt.plot(sizes, -va.mean(axis=1), marker=\"o\", label=\"CV RMSE\")\n",
    "# plt.xlabel(\"Training samples\"); plt.ylabel(\"RMSE\")\n",
    "# plt.title(\"Learning Curve (Your Best Model)\")\n",
    "# plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "# # Write 2–3 lines: are curves far apart (variance)? both high (bias)?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Lab: Tune a Regression Model for **Battery Life Prediction**\n",
    "\n",
    "**Goal:** Predict `capacity_pct` from `cycles, dod, tempC, crate, rimOhm` with the **lowest holdout RMSE** while keeping the model explainable.\n",
    "\n",
    "**Suggested path (already scaffolded in the demo):**\n",
    "\n",
    "1. Start with **Ridge** grid; record best alpha + CV RMSE.\n",
    "2. Try **PolynomialFeatures + Ridge**; watch for overfitting (degree ↑).\n",
    "3. Try **RandomForest** randomized search; compare holdout RMSE vs PolyRidge.\n",
    "4. Plot **residuals**; check patterns vs `cycles` and `tempC`.\n",
    "5. Create a short **model card** (Markdown cell): data, features, best hyperparameters, CV RMSE, holdout RMSE, limitations, next steps (e.g., add storage time, humidity, calendar aging features).\n",
    "\n",
    "---\n",
    "\n",
    "## Project Topic Prompts (pick one and scope it)\n",
    "\n",
    "* **Battery health monitoring:** Use real logs (cycles, DoD, temp profiles) to forecast **remaining useful life (RUL)**; compare Ridge vs RF vs Gradient Boosting.\n",
    "* **Solar-battery microgrid:** Predict **daily discharge depth** from weather + load; evaluate regularization impact.\n",
    "* **Sensor drift compensation:** Calibrate raw sensor output against reference; compare Polynomial Ridge vs Tree-based models.\n",
    "* **Charging optimization:** Model how **C-rate** and **thermal conditions** impact capacity fade; do controlled what-ifs.\n",
    "\n",
    "---\n",
    "\n",
    "### Wrap-up / Homework Challenge\n",
    "\n",
    "* **Nested CV** sanity check: outer 5-fold for generalization estimate, inner CV for tuning. Report outer-fold RMSE mean±std.\n",
    "* **Regularization report:** for Ridge, plot CV RMSE vs `log10(alpha)`; annotate the “sweet spot” and explain *why* it beats both under- and over-regularization.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
