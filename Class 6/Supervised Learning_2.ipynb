{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3359b787",
   "metadata": {},
   "source": [
    "# Class 6: Classification — Logistic Regression & Decision Trees (Fault Detection)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Regression estimates a value; **classification** decides a label. Today’s mission: given basic features from a signal (mean, std, gradients, threshold flags), classify whether a segment is **Normal** or **Fault** (and optionally which fault).\n",
    "\n",
    "### Lecture Notes (pocket edition)\n",
    "\n",
    "* **Binary classification**: labels {0,1}.\n",
    "* **Logistic Regression**: linear decision boundary; outputs probability via sigmoid.\n",
    "* **Decision Tree**: axis-aligned splits; handles nonlinearity; watch for overfitting → limit depth.\n",
    "* **Metrics**: accuracy (overall), precision (purity of predicted positives), recall (how many positives caught), F1 (balance), ROC-AUC (ranking quality).\n",
    "* **Confusion matrix**: TP/FP/FN/TN — vital when false alarms vs misses have different costs.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Notebook (Colab-style)\n",
    "\n",
    "### 0) Setup: create a toy “fault vs normal” dataset (self-contained)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\n",
    ")\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Simulate 2 classes using simple stats features from windows\n",
    "n = 600\n",
    "# Features: mean voltage, std voltage, slope proxy, threshold flag\n",
    "mean_normal  = np.random.normal(3.20, 0.02, n//2)\n",
    "std_normal   = np.random.normal(0.03, 0.01, n//2).clip(0.005, None)\n",
    "slope_normal = np.random.normal(0.00, 0.01, n//2)\n",
    "flag_normal  = (mean_normal > 3.6).astype(int)  # mostly 0\n",
    "\n",
    "mean_fault   = np.random.normal(3.35, 0.06, n//2)\n",
    "std_fault    = np.random.normal(0.10, 0.03, n//2).clip(0.01, None)\n",
    "slope_fault  = np.random.normal(0.03, 0.02, n//2)\n",
    "flag_fault   = (mean_fault > 3.6).astype(int)  # occasional 1s\n",
    "\n",
    "X = np.column_stack([\n",
    "    np.concatenate([mean_normal, mean_fault]),\n",
    "    np.concatenate([std_normal,  std_fault]),\n",
    "    np.concatenate([slope_normal, slope_fault]),\n",
    "    np.concatenate([flag_normal,  flag_fault])\n",
    "]).astype(float)\n",
    "\n",
    "y = np.array([0]*(n//2) + [1]*(n//2))  # 0=Normal, 1=Fault\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"mean_v\",\"std_v\",\"slope\",\"flag_high\"])\n",
    "df[\"label\"] = y\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 1) Split, scale (for LR), train Logistic Regression\n",
    "\n",
    "```python\n",
    "X = df[[\"mean_v\",\"std_v\",\"slope\",\"flag_high\"]].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=200, class_weight=None)  # try class_weight=\"balanced\" if imbalanced\n",
    "logreg.fit(X_train_s, y_train)\n",
    "\n",
    "y_prob_lr = logreg.predict_proba(X_test_s)[:,1]\n",
    "y_pred_lr = (y_prob_lr >= 0.5).astype(int)\n",
    "\n",
    "print(\"LR accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"LR precision:\", precision_score(y_test, y_pred_lr))\n",
    "print(\"LR recall:\", recall_score(y_test, y_pred_lr))\n",
    "print(\"LR F1:\", f1_score(y_test, y_pred_lr))\n",
    "print(\"LR ROC-AUC:\", roc_auc_score(y_test, y_prob_lr))\n",
    "```\n",
    "\n",
    "### 2) Train a Decision Tree (nonlinear, interpretable)\n",
    "\n",
    "```python\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_prob_dt = tree.predict_proba(X_test)[:,1]\n",
    "y_pred_dt = tree.predict(X_test)\n",
    "\n",
    "print(\"DT accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"DT precision:\", precision_score(y_test, y_pred_dt))\n",
    "print(\"DT recall:\", recall_score(y_test, y_pred_dt))\n",
    "print(\"DT F1:\", f1_score(y_test, y_pred_dt))\n",
    "print(\"DT ROC-AUC:\", roc_auc_score(y_test, y_prob_dt))\n",
    "```\n",
    "\n",
    "### 3) Confusion matrices + ROC curves\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_lr), display_labels=[\"Normal\",\"Fault\"]).plot(ax=ax[0], colorbar=False)\n",
    "ax[0].set_title(\"Logistic Regression\")\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_dt), display_labels=[\"Normal\",\"Fault\"]).plot(ax=ax[1], colorbar=False)\n",
    "ax[1].set_title(\"Decision Tree\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob_lr)\n",
    "plt.title(\"ROC — Logistic Regression\")\n",
    "plt.grid(True); plt.show()\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob_dt)\n",
    "plt.title(\"ROC — Decision Tree\")\n",
    "plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### 4) Threshold tuning (precision/recall trade-off)\n",
    "\n",
    "```python\n",
    "def evaluate_threshold(y_true, y_scores, thresh):\n",
    "    y_hat = (y_scores >= thresh).astype(int)\n",
    "    return {\n",
    "        \"thr\": thresh,\n",
    "        \"acc\": accuracy_score(y_true, y_hat),\n",
    "        \"prec\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"rec\": recall_score(y_true, y_hat),\n",
    "        \"f1\": f1_score(y_true, y_hat)\n",
    "    }\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "rows = [evaluate_threshold(y_test, y_prob_lr, t) for t in thresholds]\n",
    "pd.DataFrame(rows)\n",
    "```\n",
    "\n",
    "### 5) Peek inside the tree (feature splits)\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_tree(tree, feature_names=[\"mean_v\",\"std_v\",\"slope\",\"flag_high\"], class_names=[\"Normal\",\"Fault\"], filled=True, rounded=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Tree feature importances:\", dict(zip([\"mean_v\",\"std_v\",\"slope\",\"flag_high\"], tree.feature_importances_)))\n",
    "print(\"LR coefficients:\", dict(zip([\"mean_v\",\"std_v\",\"slope\",\"flag_high\"], logreg.coef_[0])))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Classworks (5) — Skeleton Code (students fill in)\n",
    "\n",
    "### Classwork 1: Build & split your own dataset\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 1: CUSTOM DATASET + SPLIT\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Create a small DataFrame with columns: mean_v, std_v, slope, flag_high, label.\n",
    "# 2) Do a stratified train/test split (test_size=0.3).\n",
    "# 3) Print class counts in train and test.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     \"mean_v\":  ???,\n",
    "#     \"std_v\":   ???,\n",
    "#     \"slope\":   ???,\n",
    "#     \"flag_high\": ???,\n",
    "#     \"label\":   ???  # 0/1\n",
    "# })\n",
    "# X = df[[\"mean_v\",\"std_v\",\"slope\",\"flag_high\"]].values\n",
    "# y = df[\"label\"].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(???, ???, test_size=0.3, stratify=???, random_state=0)\n",
    "# print(\"Train counts:\", np.bincount(y_train))\n",
    "# print(\"Test counts :\", np.bincount(y_test))\n",
    "```\n",
    "\n",
    "### Classwork 2: Logistic Regression with scaling + metrics\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 2: LOGISTIC REGRESSION + METRICS\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Scale features with StandardScaler.\n",
    "# 2) Fit LogisticRegression (max_iter=200).\n",
    "# 3) Compute accuracy, precision, recall, F1, ROC-AUC on TEST.\n",
    "# 4) Print classification_report.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_s = scaler.???.???(X_train)\n",
    "# X_test_s  = scaler.???(X_test)\n",
    "\n",
    "# clf = LogisticRegression(max_iter=200)\n",
    "# clf.???(X_train_s, y_train)\n",
    "\n",
    "# y_prob = clf.???(X_test_s)[:,1]\n",
    "# y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# print(\"ACC:\", ???(y_test, y_pred))\n",
    "# print(\"PREC:\", ???(y_test, y_pred))\n",
    "# print(\"REC:\",  ???(y_test, y_pred))\n",
    "# print(\"F1:\",   ???(y_test, y_pred))\n",
    "# print(\"AUC:\",  ???(y_test, y_prob))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Classwork 3: Decision Tree depth sweep\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 3: TREE DEPTH SWEEP\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Train DecisionTreeClassifier with max_depth in {2,3,4,5,6}.\n",
    "# 2) For each depth, compute test accuracy and F1; store in a table.\n",
    "# 3) Pick the \"best\" depth (justify via F1).\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# results = []\n",
    "# for d in [2,3,4,5,6]:\n",
    "#     dt = DecisionTreeClassifier(max_depth=d, random_state=0).fit(X_train, y_train)\n",
    "#     y_hat = dt.predict(X_test)\n",
    "#     results.append({\"depth\": d, \"acc\": ???(y_test, y_hat), \"f1\": ???(y_test, y_hat)})\n",
    "# import pandas as pd\n",
    "# pd.DataFrame(results)\n",
    "```\n",
    "\n",
    "### Classwork 4: Confusion matrix + ROC for your chosen model\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 4: CM + ROC\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Take your best model from CW2 or CW3.\n",
    "# 2) Plot confusion matrix and ROC curve.\n",
    "# 3) Briefly comment: is the model conservative or aggressive on positives?\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# y_prob_best = ???  # predicted probabilities for class 1\n",
    "# y_pred_best = (y_prob_best >= 0.5).astype(int)\n",
    "\n",
    "# ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_best), display_labels=[\"Normal\",\"Fault\"]).plot(colorbar=False)\n",
    "# plt.title(\"Confusion Matrix\"); plt.grid(False); plt.show()\n",
    "\n",
    "# RocCurveDisplay.from_predictions(y_test, y_prob_best)\n",
    "# plt.title(\"ROC Curve\"); plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### Classwork 5: Threshold tuning table\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 5: THRESHOLD TUNING\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) For thresholds in np.arange(0.2, 0.9, 0.1), compute precision, recall, F1.\n",
    "# 2) Make a small DataFrame \"tuning\" with columns thr, prec, rec, f1.\n",
    "# 3) Choose a threshold that prioritizes recall (explain in one sentence).\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# thresholds = np.arange(0.2, 0.9, 0.1)\n",
    "# rows = []\n",
    "# for thr in thresholds:\n",
    "#     y_hat = (y_prob_best >= thr).astype(int)\n",
    "#     rows.append({\n",
    "#         \"thr\": thr,\n",
    "#         \"prec\": ???(y_test, y_hat, zero_division=0),\n",
    "#         \"rec\":  ???(y_test, y_hat),\n",
    "#         \"f1\":   ???(y_test, y_hat),\n",
    "#     })\n",
    "# tuning = pd.DataFrame(rows)\n",
    "# tuning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Wrap-up / Homework Challenge\n",
    "\n",
    "* Re-simulate with **class imbalance** (e.g., 80% Normal, 20% Fault). Compare LR with and without `class_weight=\"balanced\"`; compare tree depths. Discuss which metric you’d report to plant ops if **missing a fault** is very costly (hint: recall / FN).\n",
    "* Bonus: add a **third class** (e.g., “Transient”) and try a **multiclass** Decision Tree; report macro-F1.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
