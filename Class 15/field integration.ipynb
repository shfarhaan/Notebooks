{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff3dadb",
   "metadata": {},
   "source": [
    "# Class 15: EE–ML Integration — Case Studies & Debugging Models\n",
    "\n",
    "**Lab:** Guided project coding\n",
    "**Project Update:** code review\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "Models aren’t magic boxes — they fail, drift, and break just like circuits. This class connects EE case studies (renewable energy forecasting, fault detection, IoT health monitoring) to ML workflows and shows systematic debugging: **data → model → evaluation → fix cycle**.\n",
    "\n",
    "---\n",
    "\n",
    "### Lecture Notes (Key Ideas)\n",
    "\n",
    "* **Case Studies:**\n",
    "\n",
    "  * Renewable energy forecasting (solar/wind).\n",
    "  * Fault detection in transformers/power lines.\n",
    "  * Predictive maintenance (vibration, thermal).\n",
    "* **Debugging Levels:**\n",
    "\n",
    "  1. **Data Debugging:** missing values, leakage, imbalance.\n",
    "  2. **Model Debugging:** bias/variance, wrong assumptions, wrong features.\n",
    "  3. **Evaluation Debugging:** wrong metric, test leakage.\n",
    "* **Debugging Tools:**\n",
    "\n",
    "  * Learning curves → bias vs variance.\n",
    "  * Residual analysis → patterns in errors.\n",
    "  * Feature importance / SHAP → interpretability.\n",
    "  * Sanity checks → shuffle labels, random baselines.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Notebook (Colab-style)\n",
    "\n",
    "### 0) Example EE Dataset: Solar power forecasting (synthetic)\n",
    "\n",
    "```python\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "np.random.seed(15)\n",
    "\n",
    "days = 365\n",
    "t = np.arange(days)\n",
    "# base sinusoidal daily pattern + trend + noise\n",
    "solar = 5 + 3*np.sin(2*np.pi*t/365*4) + 0.02*t + np.random.normal(0,0.5,days)\n",
    "# features: day, seasonality, noise\n",
    "df = pd.DataFrame({\n",
    "    \"day\": t,\n",
    "    \"tempC\": 20 + 10*np.sin(2*np.pi*t/365*2) + np.random.normal(0,1,days),\n",
    "    \"humidity\": 50 + 20*np.sin(2*np.pi*t/365) + np.random.normal(0,5,days),\n",
    "    \"solar_output\": solar\n",
    "})\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 1) Train/test split and baseline regression\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df[[\"day\",\"tempC\",\"humidity\"]]\n",
    "y = df[\"solar_output\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "rf.fit(X_train,y_train)\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test,pred,squared=False)\n",
    "print(\"Baseline RF RMSE:\", rmse)\n",
    "```\n",
    "\n",
    "### 2) Debugging — learning curves\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "sizes, train_scores, val_scores = learning_curve(rf, X_train,y_train,\n",
    "                                                 cv=5, scoring=\"neg_root_mean_squared_error\",\n",
    "                                                 train_sizes=np.linspace(0.1,1.0,6), n_jobs=-1)\n",
    "\n",
    "train_rmse = -train_scores.mean(axis=1)\n",
    "val_rmse   = -val_scores.mean(axis=1)\n",
    "\n",
    "plt.plot(sizes,train_rmse,\"o-\",label=\"Train RMSE\")\n",
    "plt.plot(sizes,val_rmse,\"o-\",label=\"CV RMSE\")\n",
    "plt.xlabel(\"Training Samples\"); plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(); plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### 3) Residual analysis\n",
    "\n",
    "```python\n",
    "resid = y_test - pred\n",
    "plt.scatter(pred,resid,alpha=0.7)\n",
    "plt.axhline(0,color=\"k\",ls=\"--\")\n",
    "plt.title(\"Residuals vs Predictions\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Residual\")\n",
    "plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### 4) Feature importance inspection\n",
    "\n",
    "```python\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "importances.sort_values().plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 5) Sanity check — shuffle labels baseline\n",
    "\n",
    "```python\n",
    "from sklearn.utils import shuffle\n",
    "y_shuff = shuffle(y_train, random_state=0)\n",
    "rf_shuff = RandomForestRegressor(n_estimators=200, random_state=0).fit(X_train,y_shuff)\n",
    "pred_shuff = rf_shuff.predict(X_test)\n",
    "rmse_shuff = mean_squared_error(y_test,pred_shuff,squared=False)\n",
    "print(\"Shuffled-label baseline RMSE:\", rmse_shuff)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Classworks (5) — Skeleton Code\n",
    "\n",
    "### Classwork 1: Debugging data issues\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 1: DATA DEBUGGING\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Load a dataset (or generate synthetic like solar).\n",
    "# 2) Check for missing values and class/label imbalance.\n",
    "# 3) Print summary stats.\n",
    "\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "# print(df.info())\n",
    "# print(df.isna().sum())\n",
    "# print(df[\"target\"].value_counts())\n",
    "```\n",
    "\n",
    "### Classwork 2: Overfitting diagnosis with learning curves\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 2: LEARNING CURVES\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Train a model (RF, Ridge, etc.).\n",
    "# 2) Compute learning curve.\n",
    "# 3) Plot train vs validation RMSE; interpret bias vs variance.\n",
    "\n",
    "# sizes, tr, va = learning_curve(model, X_train, y_train, cv=5,\n",
    "#                                scoring=\"neg_root_mean_squared_error\",\n",
    "#                                train_sizes=np.linspace(0.1,1.0,5))\n",
    "# plt.plot(sizes,-tr.mean(axis=1),label=\"Train\")\n",
    "# plt.plot(sizes,-va.mean(axis=1),label=\"CV\")\n",
    "# plt.legend(); plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### Classwork 3: Residual analysis\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 3: RESIDUALS\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Fit regression model.\n",
    "# 2) Compute residuals (y_test - y_pred).\n",
    "# 3) Scatter plot residuals vs predicted.\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# pred = model.predict(X_test)\n",
    "# resid = y_test - pred\n",
    "# plt.scatter(pred,resid)\n",
    "# plt.axhline(0,color=\"k\",ls=\"--\")\n",
    "# plt.grid(True); plt.show()\n",
    "```\n",
    "\n",
    "### Classwork 4: Feature importance or SHAP\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 4: FEATURE IMPORTANCE\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Train RandomForest or GradientBoosting model.\n",
    "# 2) Plot feature importances.\n",
    "# 3) Identify top 2 features driving predictions.\n",
    "\n",
    "# importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "# importances.sort_values().plot(kind=\"barh\")\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "### Classwork 5: Sanity check with shuffled labels\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 5: SHUFFLE BASELINE\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Shuffle target labels.\n",
    "# 2) Train model on shuffled data.\n",
    "# 3) Compare RMSE with original model; discuss why high RMSE = sanity check.\n",
    "\n",
    "# from sklearn.utils import shuffle\n",
    "# y_shuff = shuffle(y_train, random_state=0)\n",
    "# model.fit(X_train,y_shuff)\n",
    "# pred_shuff = model.predict(X_test)\n",
    "# rmse_shuff = mean_squared_error(y_test,pred_shuff,squared=False)\n",
    "# print(\"RMSE (shuffled):\", rmse_shuff)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Lab: Guided Project Coding\n",
    "\n",
    "* Pick your **project dataset** (battery, solar, comm signals, IoT, etc.).\n",
    "* Implement a **baseline model** (Linear, Ridge, RF).\n",
    "* Debug systematically:\n",
    "\n",
    "  * Plot learning curves.\n",
    "  * Residual analysis.\n",
    "  * Feature importances.\n",
    "* Write **1-page lab note**: describe bugs/issues discovered (data leaks, overfit, weak features) and fixes attempted.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Update (Code Review)\n",
    "\n",
    "* Submit your code for a **peer/instructor review**:\n",
    "\n",
    "  * Is preprocessing correct?\n",
    "  * Are train/test splits proper?\n",
    "  * Any leakage risks?\n",
    "  * Is evaluation metric appropriate?\n",
    "* Outline changes for final version (to be written up in paper draft).\n",
    "\n",
    "---\n",
    "\n",
    "### Wrap-up / Homework Challenge\n",
    "\n",
    "* Extend Classwork 4 with **SHAP values** (`shap` library) for interpretability.\n",
    "* Write a 5–6 line **debugging diary entry**: “Today my model overfit because… I fixed it by…”\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
