{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b3c00a",
   "metadata": {},
   "source": [
    "# Research Compendium: Novel ML/IoT Integration Topics in EEE\n",
    "\n",
    "---\n",
    "\n",
    "## PHASE 1: LITERATURE RECONNAISSANCE & LANDSCAPE ANALYSIS\n",
    "\n",
    "### Current Research Landscape Summary\n",
    "\n",
    "The intersection of **Machine Learning, Internet of Things, and core Electrical & Electronic Engineering domains** reveals a dynamic but **unevenly explored landscape**. \n",
    "\n",
    "**Crowded Areas (Saturation):**\n",
    "- Generic load forecasting and basic demand-side management\n",
    "- Standard predictive maintenance pipelines (CNN/LSTM for bearing fault diagnosis‚Äîwell-established)\n",
    "- Cloud-based smart grid monitoring with conventional algorithms\n",
    "- Wearable heart-rate and basic activity recognition\n",
    "\n",
    "**Emerging/Underexplored Intersections (Publication Opportunities):**\n",
    "1. **Real-time power quality event detection at distribution edge** ‚Äî limited deployment of adaptive classifiers with IoT\n",
    "2. **Federated learning for distributed fault diagnosis** ‚Äî privacy-preserving approaches absent in EEE systems\n",
    "3. **Multi-modal signal fusion with graph neural networks** ‚Äî fusion strategies underexplored vs. single-sensor deep learning\n",
    "4. **Edge-native thermal comfort modeling** ‚Äî personalized, privacy-preserving HVAC control lacking in literature\n",
    "5. **Transfer learning for heterogeneous microgrids** ‚Äî cross-domain adaptation minimal despite diverse grid types\n",
    "6. **Hierarchical RL for islanded microgrids** ‚Äî multi-agent coordination with real hardware rare\n",
    "7. **Physics-informed hybrid RUL models with IoT edge processing** ‚Äî decentralized battery monitoring underexplored\n",
    "8. **Cognitive radio with modern transformer architectures** ‚Äî spectrum sensing beyond CNN/LSTM limited\n",
    "\n",
    "---\n",
    "\n",
    "## PHASE 2: EIGHT DETAILED RESEARCH MINI-PROPOSALS\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 1: Adaptive Edge ML for Real-Time Harmonic Distortion & Power Quality Event Detection in Distribution Networks**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Conceptual Hybrid**  \n",
    "This research advances EEE by: (a) demonstrating feasibility of **real-time, edge-deployed power quality classification** using lightweight ML, (b) addressing the gap between centralized SCADA systems (expensive, slow) and distributed DSM needs, and (c) contributing practical **IoT-enabled condition monitoring** for grid modernization.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Lightweight Wavelet-Neural Network Framework for Edge-Deployed Power Quality Disturbance Detection in Low-Voltage Networks*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "Existing power quality detection systems rely on centralized analysis (PMU + SCADA) or offline classification, missing real-time, resource-constrained edge deployment. Recent work [1] (Islam et al., 2025) reviews ML for power stability but does not address IoT edge inference. Reference [2] (Rathore et al., 2025) proposes wavelet feature extraction but lacks edge optimization or hardware validation.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*How can Discrete Wavelet Transform (DWT) feature extraction combined with lightweight neural networks (e.g., ELM, quantized NN) achieve sub-100ms latency power quality classification on resource-constrained IoT gateways while maintaining ‚â•95% accuracy across multiple harmonics and sag/swell events?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Deploy a trainable, edge-optimized power quality event classifier on Arduino/ESP32-based IoT nodes that detects and classifies harmonics, voltage sags, swells, and transients in <100ms with minimal memory footprint (<2 MB model).\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. How does model quantization (INT8, binary networks) impact classification accuracy vs. inference latency?\n",
    "2. Can transfer learning from synthetic power signals reduce field training data requirements?\n",
    "3. What is the optimal DWT decomposition level (Db4/Db6) for real-time feature extraction on edge hardware?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Discrete Wavelet Transform decomposition:**\n",
    "\\[ s_d(n) = \\sum_{k} c_j(k) \\psi(2^j n - k) + \\sum_{k} d_j(k) \\phi(2^j n - k) \\]\n",
    "where \\(s_d(n)\\) is the voltage signal, \\(\\psi, \\phi\\) are wavelet and scaling functions, and approximation/detail coefficients are extracted at decomposition level \\(j\\).\n",
    "\n",
    "**Feature vector construction:**\n",
    "\\[ \\mathbf{F} = [\\text{Energy}(D_1), \\text{Energy}(D_2), \\ldots, \\text{Entropy}(A_j), \\text{THD}, \\text{Frequency}] \\]\n",
    "\n",
    "**Neural Network Loss (multi-class classification):**\n",
    "\\[ L = -\\sum_{c=1}^{C} y_c \\log(\\hat{p}_c) + \\lambda \\|W\\|_2 \\]\n",
    "\n",
    "**üåü Innovation (2‚Äì3 lines):**  \n",
    "First practical edge-deployed power quality classifier combining DWT with quantized neural networks for <100ms latency on consumer-grade microcontrollers; introduces transfer learning protocol using synthetic power signals to reduce labeled field data dependency; demonstrates real-time IoT-to-cloud integration with local confidence thresholding for alarm prioritization.\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- **Hardware:** Arduino Mega 2560, ESP32-WROOM-32, Raspberry Pi Zero 2W (edge nodes); power quality sensors (voltage/current transducers); LoRa or MQTT gateway\n",
    "- **Software:** TensorFlow Lite, MATLAB/Python (signal processing), edge ML frameworks (TensorFlow.js, PyTorch Mobile)\n",
    "- **Protocol:** MQTT for edge-cloud communication; SQLite for local data buffering\n",
    "\n",
    "**üìä Metrics & Evaluation Plan:**\n",
    "\n",
    "| Metric | Target | Validation Method |\n",
    "|--------|--------|-------------------|\n",
    "| Classification Accuracy | ‚â•95% | 5-fold cross-validation on IEEE test signals + real grid data |\n",
    "| Latency (edge inference) | <100 ms | Profiling on target hardware; measure end-to-end (sensor ‚Üí classification) |\n",
    "| Model Size | <2 MB | Quantization + pruning; test on device storage |\n",
    "| F1-Score (harmonic-distorted class) | ‚â•0.92 | Macro-average F1 across all 7 disturbance types |\n",
    "| False Positive Rate (alarm fatigue) | <5% | 30-day continuous field monitoring |\n",
    "| Power Consumption (inference) | <50 mW | Energy profiling on ESP32 @80 MHz |\n",
    "\n",
    "**Statistical Validation:** Paired t-tests comparing edge vs. cloud accuracy; ROC-AUC analysis for disturbance presence/absence; Kappa coefficient for inter-observer agreement (grid operator manual verification vs. model).\n",
    "\n",
    "**üß™ Detailed Methodology:**\n",
    "\n",
    "- **Model/Algorithm:**\n",
    "  1. DWT decomposition: Daubechies 4 (Db4), 4-level decomposition of raw voltage waveform (sampling rate 10 kHz)\n",
    "  2. Feature extraction: 23-dimensional feature vector (energy, entropy, peak, RMS per band + THD + frequency)\n",
    "  3. Classifier options (rank by performance/latency):\n",
    "     - Extreme Learning Machine (ELM) ‚Äî <50 ms training, fast inference\n",
    "     - Quantized MLP (INT8 weights) ‚Äî 85% smaller than FP32 baseline\n",
    "     - Gradient Boosted Trees (XGBoost on embedded) ‚Äî compact, interpretable\n",
    "  4. Training: 70‚Äì15‚Äì15 split (train‚Äìval‚Äìtest) on combined synthetic (IEEE PQ test signals) + real grid data\n",
    "\n",
    "- **Data Acquisition:**\n",
    "  - **Synthetic data:** MATLAB-generated power signals (FAULTS toolbox + IEEE Std. 1159) ‚Äî 100 instances per disturbance class (sag, swell, harmonics, transients, notches, flicker, interruption)\n",
    "  - **Real data:** Deploy pilot on 5 distribution feeders (industrial/residential) for 6 months; collect 10 kHz samples, 128-sample windows (12.8 ms) ‚Äî target ‚â•50K labeled samples\n",
    "  - **Data source:** Grid operator SCADA validation; expert manual labeling via power engineer review\n",
    "\n",
    "- **Hardware Integration:**\n",
    "  - **Sensors:** Voltage transformer (0‚Äì500 V ‚Üí 0‚Äì3.3 V input) + burden resistor; current sensors (optional)\n",
    "  - **Edge node:** ESP32 with 4 MB SPRAM; ADC at 10-bit 10 kHz sampling\n",
    "  - **Communication:** MQTT publish (disturbance class + confidence) every 1 sec to local gateway; gateway aggregates for cloud trend analysis\n",
    "  - **Cloud backend:** Node-RED or AWS IoT Core for time-series logging; Grafana for operator dashboard\n",
    "\n",
    "- **Training/Testing Workflow:**\n",
    "  1. Data preprocessing: Standardization (zero-mean, unit variance); low-pass filter (cutoff ~5 kHz)\n",
    "  2. DWT feature extraction on training set; compute statistical feature selection (chi-square for class correlation)\n",
    "  3. Train base model on 70% data; hyperparameter tuning via 5-fold CV on validation set\n",
    "  4. Model quantization: Convert FP32 ‚Üí INT8 using TensorFlow Lite quantization-aware training\n",
    "  5. Edge deployment: Flash .tflite model to ESP32 flash memory; run inference on real-time sensor stream\n",
    "  6. Field validation: 30-day continuous monitoring; compare edge classifications with grid operator manual logs\n",
    "\n",
    "- **Ablation Studies:**\n",
    "  1. Wavelet choice: Compare Db4, Db6, Coiflet 3 ‚Äî measure accuracy drop vs. latency gain\n",
    "  2. Feature subset: Remove energy/entropy/THD individually; quantify accuracy penalty\n",
    "  3. Quantization impact: INT16, INT8, binary networks ‚Äî latency vs. accuracy trade-off curve\n",
    "  4. Transfer learning effectiveness: Train on synthetic data, fine-tune on field data (1%, 5%, 10% labels); measure data efficiency\n",
    "\n",
    "- **Error & Reliability Analysis:**\n",
    "  - Confusion matrices per disturbance class; identify misclassified pairs (e.g., sag vs. frequency deviation)\n",
    "  - Residual analysis: Plot prediction confidence distribution; set adaptive threshold for low-confidence alerts\n",
    "  - Sensitivity analysis: Vary sensor noise (SNR 20‚Äì50 dB); test robustness\n",
    "  - Device accuracy over time: Monthly recalibration check; monitor for concept drift\n",
    "\n",
    "- **Reproducibility:**\n",
    "  - Code repository: GitHub with full data pipeline (synthetic generation, training, quantization)\n",
    "  - Environment: Docker container (Python 3.9, TensorFlow 2.12, MATLAB Runtime)\n",
    "  - Seed control: Fix random seeds; document all hyperparameters in YAML config\n",
    "  - Dataset: Release anonymized real-grid subset (1000 labeled samples) on IEEE Dataport under CC-BY-NC license\n",
    "  - Hardware setup: Detailed schematic and bill of materials (BoM) for ESP32 + sensor PCB\n",
    "\n",
    "**‚öì Dataset Creation:**\n",
    "\n",
    "- **Synthetic:** 700 power waveform samples (100 instances √ó 7 disturbance classes: harmonic, sag, swell, transient, notch, flicker, interruption)\n",
    "- **Sampling rate:** 10 kHz (128 samples/window = 12.8 ms)\n",
    "- **Annotation:** Disturbance type, magnitude (%), duration (cycles), harmonics present (THD %)\n",
    "- **Field collection:** 6-month pilot on 5 low-voltage distribution nodes (> 50K windows collected)\n",
    "\n",
    "**üöß Use of Existing Datasets:**\n",
    "- IEEE PQ Test Signals (synthetic reference library) for training base model\n",
    "- REDD (household electricity demand data) adapted for sag/swell patterns\n",
    "- DRPLDATA (distribution network dataset) for real-world harmonic profiles\n",
    "\n",
    "**üß∞ Programming Needs:**\n",
    "\n",
    "| Component | Required/Optional | Notes |\n",
    "|-----------|------------------|-------|\n",
    "| MATLAB/Python preprocessing | Required | Signal processing, wavelet decomposition |\n",
    "| TensorFlow Lite | Required | Model quantization & edge deployment |\n",
    "| Arduino IDE / PlatformIO | Required | Firmware for ESP32 edge node |\n",
    "| MQTT broker (Mosquitto) | Required | Local edge-cloud messaging |\n",
    "| Grafana | Optional | Real-time monitoring dashboard |\n",
    "\n",
    "**Detailed Plan:**\n",
    "1. Weeks 1‚Äì2: Data acquisition setup, synthetic signal generation (MATLAB)\n",
    "2. Weeks 3‚Äì4: DWT feature extraction pipeline, baseline model training (XGBoost, ELM)\n",
    "3. Weeks 5‚Äì6: Model quantization, embedded inference optimization (TensorFlow Lite)\n",
    "4. Weeks 7‚Äì8: Field deployment, edge-cloud integration, validation & paper drafting\n",
    "\n",
    "**üì¢ 8-Week Comprehensive Roadmap:**\n",
    "\n",
    "| Week | Milestone | Deliverables |\n",
    "|------|-----------|--------------|\n",
    "| 1 | Data & environment setup | MATLAB signal generator, dataset folder structure |\n",
    "| 2 | Synthetic data generation | 700 labeled PQ samples (all disturbance types) |\n",
    "| 3 | DWT feature extraction | Feature vector CSV (1K samples √ó 23 features) |\n",
    "| 4 | Model training | Baseline ELM/XGBoost (95%+ val accuracy) |\n",
    "| 5 | Quantization & optimization | INT8 model, latency benchmark on ESP32 |\n",
    "| 6 | Edge firmware & MQTT | Functional ESP32 firmware, local MQTT integration |\n",
    "| 7 | Field pilot deployment | 30-day live data collection, operator feedback |\n",
    "| 8 | Analysis & paper draft | Confusion matrices, ROC curves, paper submission draft |\n",
    "\n",
    "**üíª Compute Feasibility:**\n",
    "- **GPU training:** ~2‚Äì4 hours on Colab T4 GPU (feature extraction + XGBoost hyperparameter tuning)\n",
    "- **Memory:** 4 GB RAM sufficient (dataset < 500 MB)\n",
    "- **Edge inference:** <50 mW on ESP32; no GPU required\n",
    "- **Fallback:** CPU-only training on laptop; ~12 hours without GPU\n",
    "\n",
    "### C. Ethics & Data Governance\n",
    "- **Data Privacy:** Anonymize grid node identifiers; store raw sensor data locally, transmit only aggregated statistics\n",
    "- **Security:** MQTT over TLS; restrict edge node MQTT topics by role (read-only for monitoring, write-only for grid ops)\n",
    "- **Responsible Usage:** Ensure false alarms do not trigger unnecessary maintenance; document uncertainty margins for operators\n",
    "- **Environmental:** Edge processing reduces cloud bandwidth by ~90%, supporting energy-efficient IoT operations\n",
    "\n",
    "### D. Publication Targeting\n",
    "\n",
    "**Suitable Venues:**\n",
    "- **IEEE Sensors Journal** ‚Äî emphasis on edge-deployed sensing + ML\n",
    "- **IEEE Transactions on Industrial Electronics** ‚Äî power quality + embedded systems\n",
    "- **Elsevier Energy AI** ‚Äî grid modernization + AI\n",
    "\n",
    "**Reference Papers (style/expectations):**\n",
    "1. Islam et al. (2025), *Results in Engineering*, \"Machine learning for power system stability and control\" ‚Äî reviews RL/SVM for grid control; shows empirical validation on CIGRE test network\n",
    "2. Rathore et al. (2025), *Sensors*, \"Power quality event detection and classification using wavelet alienation algorithm\" ‚Äî DWT-based detection; compares classical vs. ML\n",
    "3. Molu et al. (2024), *Frontiers Energy Research*, \"Enhancing power quality monitoring with discrete wavelet\" ‚Äî real-time DWT + ELM; validates on industrial feeders\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 2: Federated Learning for Fault Diagnosis in Wireless Sensor Networks with Privacy-Preserving Data Processing**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Methodological Hybrid**  \n",
    "This research advances EEE by: (a) introducing **privacy-preserving collaborative learning** across distributed IoT sensor networks without centralizing sensitive industrial data, (b) addressing **communication efficiency** in bandwidth-constrained edge IoT environments, and (c) enabling **heterogeneous fault detection** across diverse equipment types (motors, bearings, transformers) within a single federated framework.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Adaptive Federated Learning Framework for Distributed Fault Diagnosis in IoT-Enabled Industrial Wireless Sensor Networks*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "Standard centralized ML requires uploading raw sensor data to cloud, raising privacy/security concerns in critical infrastructure. Recent federated learning work [1] (Vahabi et al., 2025) reviews FL in IIoT but focuses on communication efficiency; privacy mechanisms and fault diagnosis applications remain underexplored. Reference [2] (Prasad et al., 2023) proposes WSN fault detection using DBN but assumes centralized data; distributed training is absent.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*How can a federated learning framework with adaptive model aggregation and selective data compression enable accurate multi-class fault diagnosis across heterogeneous IoT sensor nodes while achieving >90% accuracy, reducing communication overhead by >80%, and maintaining privacy (no raw sensor data leaves the edge)?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Develop a **privacy-preserving, communication-efficient federated learning system** where 20‚Äì50 distributed sensor nodes collaboratively train a fault diagnosis model (motor, bearing, transformer faults) without sharing raw sensor data; achieve 95%+ accuracy with <5% communication overhead vs. centralized approach.\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. How does non-IID (non-independent, identically distributed) sensor data heterogeneity across factory sites impact model convergence and final accuracy?\n",
    "2. Can selective gradient compression (sparsification, quantization) reduce communication bandwidth by >80% with <5% accuracy loss?\n",
    "3. What privacy guarantees (differential privacy epsilon) are sufficient to prevent membership inference attacks on the federated model?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Federated Averaging (FedAvg) loss at round \\(t\\):**\n",
    "\\[ L^t = \\frac{1}{N} \\sum_{i=1}^{N} n_i \\cdot L_i(\\mathbf{w}^t) \\]\n",
    "where \\(N\\) = number of nodes, \\(n_i\\) = samples on node \\(i\\), \\(\\mathbf{w}^t\\) = global model weights at round \\(t\\).\n",
    "\n",
    "**Gradient compression (top-K sparsification):**\n",
    "\\[ \\tilde{\\mathbf{g}}_i = \\text{TopK}(\\mathbf{g}_i, k) \\quad \\text{where} \\quad k = \\lceil C \\cdot \\text{dim}(\\mathbf{g}_i) \\rceil \\]\n",
    "with compression ratio \\(C \\in [0.01, 0.1]\\); reconstruct sparse gradient at server via zero-padding.\n",
    "\n",
    "**Differential Privacy (DP-FedAvg):**\n",
    "\\[ L^t_{\\text{DP}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Clip}(\\mathbf{g}_i, \\Delta) + \\mathcal{N}(0, \\sigma^2 \\Delta^2) \\]\n",
    "where \\(\\Delta\\) = clipping threshold, \\(\\sigma\\) scaled for privacy budget \\(\\epsilon\\).\n",
    "\n",
    "**Multi-task fault classification loss:**\n",
    "\\[ L_{\\text{multi}} = -\\sum_{c=1}^{C} y_c \\log(\\hat{p}_c) + \\alpha \\cdot \\text{KL}[\\text{server priors} || \\text{node posteriors}] \\]\n",
    "\n",
    "**üåü Innovation (2‚Äì3 lines):**  \n",
    "First federated learning application for cross-site industrial fault diagnosis combining gradient compression + differential privacy without accuracy degradation; introduces adaptive aggregation weights based on local data quality scores; demonstrates real-time decentralized inference with sub-second latency on embedded edge gateways.\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- **Hardware:** Raspberry Pi 4 (edge gateway per site), Arduino/ESP32 (sensor nodes), LoRa or WiFi for inter-node communication\n",
    "- **Software:** FedML (federated ML framework), TensorFlow Privacy, Flower (FL orchestration platform)\n",
    "- **Protocol:** MQTT with TLS for encrypted communication; optional blockchain for audit trail (lightweight, e.g., Hyperledger Fabric)\n",
    "\n",
    "**üìä Metrics & Evaluation Plan:**\n",
    "\n",
    "| Metric | Target | Validation Method |\n",
    "|--------|--------|-------------------|\n",
    "| Global Accuracy | ‚â•95% | 5-fold CV across all nodes; confusion matrix per fault class |\n",
    "| Communication Reduction | >80% | Measure total bytes transmitted vs. centralized baseline |\n",
    "| DP Privacy Budget (Œµ) | <5.0 | Compute via RDP composition; test membership inference attack |\n",
    "| Convergence Speed | <50 rounds | Monitor loss over FL rounds; compare vs. centralized SGD |\n",
    "| Local Accuracy Drift (heterogeneity) | <3% | Std deviation of per-node accuracy during training |\n",
    "| Inference Latency (edge) | <500 ms | End-to-end time from sensor read ‚Üí fault prediction on RPi4 |\n",
    "\n",
    "**Statistical Validation:** Friedman test for significance of communication reduction; Kaplan-Meier curves for fault detection sensitivity over time.\n",
    "\n",
    "**üß™ Detailed Methodology:**\n",
    "\n",
    "- **Model/Algorithm:**\n",
    "  1. **Global model architecture:** CNN (3 conv layers, 128 filters Db4) + LSTM (128 units) for temporal fault patterns\n",
    "  2. **Local model:** Smaller CNN-LSTM (2 conv, 64 units LSTM) for resource-constrained nodes\n",
    "  3. **Aggregation:** FedAvg with adaptive weighting; per-node weight \\(w_i = n_i / \\sum_j n_j\\)\n",
    "  4. **Gradient compression:** TopK sparsification (keep top 10% gradients by magnitude)\n",
    "  5. **Differential privacy:** Gaussian mechanism, \\(\\epsilon = 5.0\\) per round (cumulative over 100 rounds ‚Üí \\(\\epsilon_{\\text{total}} \\approx 0.5\\) via RDP)\n",
    "  6. **Local training:** 5 epochs per round; SGD with momentum (0.9)\n",
    "\n",
    "- **Data Acquisition:**\n",
    "  - **Sensor types:** Vibration (accelerometer 100 Hz‚Äì10 kHz), temperature, current (for motor), voltage\n",
    "  - **Fault sources:** 3 factory sites with diverse equipment (10 motors, 15 bearings, 5 transformers)\n",
    "  - **Data format:** 30-second windows, 5-min intervals ‚Üí 600 samples/day/node = 180K total (6 months)\n",
    "  - **Labeling:** Site engineers mark fault onset + type; create 7 classes: normal, bearing outer-race fault, inner-race fault, motor winding fault, transformer thermal anomaly, bearing cage fault, combined faults\n",
    "\n",
    "- **Hardware Integration:**\n",
    "  - **Sensor nodes:** MPU6050 (vibration), DS18B20 (temperature), ACS712 (current) ‚Üí Arduino/ESP32 with local data logging (microSD)\n",
    "  - **Edge gateway:** Raspberry Pi 4, 4 GB RAM; runs FL client + local inference engine\n",
    "  - **Communication:** LoRa (868 MHz, 10 km range) for inter-site, WiFi for intra-site; encrypted MQTT for FL round coordination\n",
    "  - **Cloud (optional):** AWS IoT Core for global aggregation server; central model checkpoint storage\n",
    "\n",
    "- **Training/Testing Workflow:**\n",
    "  1. **Setup:** Deploy FL client code on each RPi4 gateway; configure MQTT broker; define model architecture\n",
    "  2. **Round initialization:** Server sends global weights \\(\\mathbf{w}^t\\) to selected subset of clients (e.g., 10/20)\n",
    "  3. **Local training:** Each client trains on local data (5 epochs); computes gradients; applies TopK compression\n",
    "  4. **Upload:** Compressed gradients ‚Üí server (low bandwidth: ~1‚Äì2 MB vs. 50 MB raw)\n",
    "  5. **Aggregation:** Server computes FedAvg, adds DP noise, broadcasts new weights\n",
    "  6. **Repeat:** 100 FL rounds (~2‚Äì3 weeks per training cycle with daily data refresh)\n",
    "  7. **Validation:** Every 10 rounds, evaluate on hold-out test set (not shared with FL training) at each site\n",
    "\n",
    "- **Ablation Studies:**\n",
    "  1. **Compression impact:** Compare TopK (10%, 5%, 1%) vs. no compression ‚Üí accuracy loss vs. communication gain\n",
    "  2. **Privacy budget:** Vary Œµ (1.0, 5.0, 10.0) ‚Üí measure membership inference attack success rate\n",
    "  3. **Heterogeneity effect:** Train on IID vs. non-IID splits ‚Üí show convergence slowdown; quantify via \"non-IIDness\" parameter \\(\\rho\\)\n",
    "  4. **Aggregation weight:** Fixed average vs. adaptive quality-weighted ‚Üí compare final model accuracy\n",
    "\n",
    "- **Error & Reliability Analysis:**\n",
    "  - Per-site confusion matrices; identify which sites have model drift\n",
    "  - Communication failure resilience: Simulate node offline (5%, 10% probability) ‚Üí check fault tolerance\n",
    "  - Adversarial robustness: Inject poisoned gradients from one node; measure global model accuracy drop\n",
    "  - Temporal drift: Retrain every month; monitor accuracy over 12 months for concept drift\n",
    "\n",
    "- **Reproducibility:**\n",
    "  - FedML/Flower code on GitHub with all hyperparameters\n",
    "  - Synthetic heterogeneous dataset (UCI bearing fault data + artificially partitioned across 20 virtual clients)\n",
    "  - Docker container with FL server + client environment\n",
    "  - Configuration YAML for easy parameter adjustment\n",
    "\n",
    "**‚öì Dataset Creation:**\n",
    "\n",
    "- **Per-site data volume:** ~180K labeled 30-sec windows per month (30 days √ó 200 samples/day)\n",
    "- **Annotation:** Fault class + severity level (1‚Äì5); manual labeling by domain expert\n",
    "- **Sampling rate:** 10 kHz (vibration), 1 Hz (thermal, current)\n",
    "- **Duration:** Collect 6 months across 3 sites; create train/val/test splits (70/15/15) preserving site stratification\n",
    "\n",
    "**üöß Use of Existing Datasets:**\n",
    "- **UCI Machine Learning Bearing Dataset** ‚Äî adapt by simulating non-IID partitions across 20 virtual clients\n",
    "- **CWRU Bearing Fault Database** ‚Äî baseline for validation on known fault types\n",
    "- **Motor Current Signature Analysis (MCSA) dataset** ‚Äî create synthetic aggregation across 10 motor types\n",
    "\n",
    "**üß∞ Programming Needs:**\n",
    "\n",
    "| Component | Required/Optional | Notes |\n",
    "|-----------|------------------|-------|\n",
    "| FedML / Flower framework | Required | FL orchestration, gradient compression, privacy mechanisms |\n",
    "| TensorFlow Privacy | Required | DP-SGD implementation |\n",
    "| MQTT broker (Mosquitto) | Required | Inter-node communication |\n",
    "| Arduino IDE / Python | Required | Sensor node firmware + RPi4 edge gateway |\n",
    "| Jupyter notebooks | Optional | Hyperparameter tuning, visualization |\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone | Deliverables |\n",
    "|------|-----------|--------------|\n",
    "| 1 | Environment setup | FedML/Flower installed; synthetic heterogeneous dataset created |\n",
    "| 2 | Model design | CNN-LSTM architecture defined; local training baseline on centralized data |\n",
    "| 3 | FL framework | FedAvg implementation; test on small network (3 clients) |\n",
    "| 4 | Gradient compression | TopK sparsification coded; communication reduction measured |\n",
    "| 5 | DP integration | DP-SGD implemented; privacy epsilon calculated |\n",
    "| 6 | Hardware integration | RPi4 firmware + MQTT; run FL with real sensor streams |\n",
    "| 7 | Scalability testing | Scale to 20 nodes; measure convergence, latency, privacy |\n",
    "| 8 | Validation & paper | Cross-site testing, robustness analysis, paper draft |\n",
    "\n",
    "**üíª Compute Feasibility:**\n",
    "- **FL training:** ~20‚Äì30 hours on Colab GPU (100 rounds, 20 clients, 5 epochs each)\n",
    "- **Edge inference:** <500 ms per fault diagnosis on RPi4 (no GPU required)\n",
    "- **Memory:** 8 GB RAM for central aggregation server; 4 GB per edge gateway\n",
    "- **Fallback:** Simulate federated training on laptop using 5 synthetic clients; reduced dataset\n",
    "\n",
    "### C. Ethics & Data Governance\n",
    "- **Privacy-First Design:** Raw sensor data never leaves factory site; only encrypted gradients transmitted\n",
    "- **Data Minimization:** Gradient compression reduces data volume by 90%; temporal sampling reduces frequency\n",
    "- **Auditability:** Blockchain (optional) logs all FL rounds, model versions, privacy audits\n",
    "- **Consent:** Factory operators explicitly authorize federated participation; can withdraw anytime\n",
    "\n",
    "### D. Publication Targeting\n",
    "\n",
    "**Suitable Venues:**\n",
    "- **IEEE Internet of Things Journal** ‚Äî emphasizes IoT-ML integration + privacy\n",
    "- **Springer Neural Computing & Applications** ‚Äî federated learning methodologies\n",
    "- **Elsevier Computers & Electrical Engineering** ‚Äî industrial embedded ML\n",
    "\n",
    "**Reference Papers:**\n",
    "1. Vahabi et al. (2025), *Elsevier Future Generation Computer Systems*, \"Federated learning at the edge in Industrial Internet of Things\" ‚Äî reviews FL in IIoT; identifies communication as bottleneck\n",
    "2. Prasad et al. (2023), *IEEE Xplore*, \"Algorithms for Fault Detection and Diagnosis in Wireless Sensor Networks Using Deep Learning\" ‚Äî centralized WSN fault detection; shows DBN superiority\n",
    "3. Alasbali et al. (2025), *Frontiers in Computer Science*, \"Integrating federated learning in an IoT-enabled edge-computing environment\" ‚Äî FL on edge devices; privacy mechanisms\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 3: Multi-Modal Sensor Fusion with Graph Neural Networks for Rolling Bearing Fault Diagnosis**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Methodological**  \n",
    "This research advances EEE by: (a) introducing **graph neural networks (GNNs) for multi-modal fusion** ‚Äî moving beyond simple concatenation or shallow fusion to learn **topology of fault propagation** through sensor relationships, (b) demonstrating **data efficiency** via fusion (fewer faulty samples needed vs. single-sensor baselines), and (c) enabling **interpretable fault localization** (which bearing component is failing?) through graph attention mechanisms.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Graph Neural Network-based Multi-Modal Sensor Fusion for Robust Rolling Bearing Fault Localization and Severity Assessment*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "Deep learning for bearing fault diagnosis is mature (CNN/LSTM), but existing work treats each sensor independently or uses naive fusion (concatenation). Reference [1] (Chennana et al., 2025) fuses shallow + deep features but via score-level, not considering sensor interdependencies. Reference [2] (Wang et al., 2024) uses multi-sensor CNN-LSTM but lacks graph-based topology learning; cannot explain which sensor modality drives fault detection.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*Can a GNN-based multi-modal fusion architecture, treating sensors as nodes and fault-propagation paths as edges, achieve >98% bearing fault classification accuracy with 40% fewer labeled faulty samples vs. single-modality CNNs, while providing interpretable attention weights indicating fault origin (outer-race, inner-race, cage)?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Develop a **GNN-based fault diagnosis framework** combining vibration (accelerometer), temperature, acoustic emission (AE), and motor current signature analysis (MCSA) data on a learnable sensor graph; achieve state-of-the-art accuracy while reducing data annotation burden and providing explainable predictions.\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. How does the intra-modal correlation graph (e.g., wavelet frequencies as nodes, coupling as edges) improve fault localization vs. inter-modal fusion alone?\n",
    "2. Can attention mechanisms in GNN reveal which sensor combinations are most informative for each fault type?\n",
    "3. How does adding adversarial training (domain adversarial GNN) improve robustness to sensor noise or miscalibration?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Graph node embedding (sensor features):**\n",
    "\\[ \\mathbf{h}_i^{(0)} = \\mathbf{x}_i \\in \\mathbb{R}^{d} \\quad \\text{(raw sensor features)} \\]\n",
    "\n",
    "**Graph Attention Network (GAT) layer:**\n",
    "\\[ \\mathbf{h}_i^{(\\ell+1)} = \\sigma \\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\mathbf{h}_j^{(\\ell)} \\right) \\]\n",
    "where attention coefficient:\n",
    "\\[ \\alpha_{ij}^{(\\ell)} = \\frac{\\exp(\\text{LeakyReLU}(\\mathbf{a}^T [\\mathbf{W} \\mathbf{h}_i || \\mathbf{W} \\mathbf{h}_j]))}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(\\text{LeakyReLU}(\\mathbf{a}^T [\\mathbf{W} \\mathbf{h}_i || \\mathbf{W} \\mathbf{h}_k]))} \\]\n",
    "\n",
    "**Wavelet-based intra-modal graph construction:**\n",
    "\\[ G_{\\text{intra}} = (V, E) \\quad \\text{where} \\quad V = \\{\\text{freq bands}\\}, \\quad E_{ij} = |C(f_i, f_j)| > \\tau \\]\n",
    "\\(C(f_i, f_j)\\) = cross-correlation of wavelet energies at frequencies \\(f_i, f_j\\); \\(\\tau\\) = threshold.\n",
    "\n",
    "**Multi-task loss (classification + severity regression):**\n",
    "\\[ L = L_{\\text{cls}}(\\hat{y}, y) + \\lambda L_{\\text{reg}}(\\hat{s}, s) + \\beta \\cdot \\text{KL}[q(z) || p(z)] \\]\n",
    "where \\(\\hat{s}\\) = predicted fault severity (0‚Äì5), \\(z\\) = latent embedding.\n",
    "\n",
    "**üåü Innovation (2‚Äì3 lines):**  \n",
    "First GNN-based bearing fault diagnosis integrating vibration + thermal + AE + current in learnable sensor graph; demonstrates fault localization capability via attention visualization; achieves 40% data efficiency gain vs. single-sensor CNNs; introduces adversarial training for robustness to sensor noise.\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- **Hardware:** Vibration sensors (ADXL345), thermistors (NTC 10K), acoustic emission transducers (15 kHz‚Äì1 MHz), clamp meter (current) on 3‚Äì10 HPmotors; Raspberry Pi 4 for data aggregation\n",
    "- **Software:** PyTorch Geometric (GNNs), scikit-learn (preprocessing), TensorFlow (adversarial training)\n",
    "\n",
    "**üìä Metrics & Evaluation Plan:**\n",
    "\n",
    "| Metric | Target | Validation Method |\n",
    "|--------|--------|-------------------|\n",
    "| Classification Accuracy | ‚â•98% | 5-fold CV on CWRU + lab-collected multi-modal data |\n",
    "| Data Efficiency | 40% fewer labels | Compare annotation cost: GNN vs. CNN baseline |\n",
    "| Fault Localization Accuracy | ‚â•92% (outer/inner/cage) | Confusion matrix for 3-class spatial localization |\n",
    "| Attention Interpretability | Sensitivity > 0.85 | Grad-CAM on top attention heads; compare with domain expert judgment |\n",
    "| Adversarial Robustness | Accuracy drop <5% | Add Gaussian noise (SNR 20 dB); test accuracy degradation |\n",
    "| Inference Latency | <200 ms | Profile on RPi4 with 4 sensors |\n",
    "\n",
    "**Statistical Validation:** McNemar's test for significance of accuracy improvement vs. baseline; sensitivity/specificity analysis per fault class.\n",
    "\n",
    "**üß™ Detailed Methodology:**\n",
    "\n",
    "- **Model/Algorithm:**\n",
    "  1. **Sensor graph:** 6 nodes (vibration X/Y/Z, temperature, AE, current)\n",
    "  2. **Intra-modal graph:** Wavelet frequency bands as 16 nodes; edges = cross-correlation >0.5\n",
    "  3. **GNN architecture:** 3-layer GAT (64 ‚Üí 128 ‚Üí 64 hidden dims); multi-head attention (4 heads)\n",
    "  4. **Output head:** Classification (7 classes: normal + 6 fault types) + severity regression\n",
    "  5. **Adversarial domain adaptation:** Add discriminator (2-layer MLP) to distinguish lab vs. field data\n",
    "\n",
    "- **Data Acquisition:**\n",
    "  - **Lab setup:** Run 3‚Äì10 motors (0.5‚Äì5 HP) from healthy to failure (~2‚Äì6 weeks)\n",
    "  - **Sampling:** 10 kHz (vibration), 1 Hz (thermal), 40 kHz (AE), 100 Hz (current)\n",
    "  - **Fault types:** 6 classes (normal, outer-race, inner-race, cage, combined, severe)\n",
    "  - **Duration:** Collect 3‚Äì6 months of continuous data; annotate 500 samples per class (3,500 total)\n",
    "\n",
    "- **Hardware Integration:**\n",
    "  - **Sensors:** ADXL345 (vibration, ¬±16 g), thermistors (‚àí20‚Äì100¬∞C), Knowles FlexPower acoustic sensors (KPPC-3P8), SCT-013 current transformer\n",
    "  - **Data logger:** Raspberry Pi 4 with USB DAQ (ADS1115 for analog) + I2C hub\n",
    "  - **Storage:** Local microSD (128 GB) for buffering; rsync to cloud server\n",
    "\n",
    "- **Training/Testing Workflow:**\n",
    "  1. **Preprocessing:** Normalize each sensor stream independently (z-score)\n",
    "  2. **Feature extraction:** Wavelet decomposition (Db6, 5 levels) for vibration; statistical features (mean, std, skew) for thermal/current; raw waveform for AE\n",
    "  3. **Graph construction:** Build intra-modal frequency graph from wavelet correlations; inter-modal edges via mutual information\n",
    "  4. **Train/val/test split:** 70/15/15 stratified by fault type\n",
    "  5. **Training:** Adam optimizer, learning rate 0.001, 100 epochs, early stopping on validation accuracy\n",
    "  6. **Adversarial training:** Alternate between GNN and discriminator (5 GNN steps, 1 discriminator step)\n",
    "  7. **Evaluation:** Confusion matrix, ROC-AUC per class, attention visualization\n",
    "\n",
    "- **Ablation Studies:**\n",
    "  1. **Graph topology:** Full inter+intra-modal vs. inter-modal only vs. single-sensor CNN\n",
    "  2. **Attention heads:** 1, 4, 8 heads ‚Üí accuracy + inference latency trade-off\n",
    "  3. **Adversarial training:** With/without domain adversary ‚Üí test generalization to unseen motor types\n",
    "  4. **Wavelet choice:** Db4 vs. Db6 vs. Morlet ‚Üí localization accuracy comparison\n",
    "\n",
    "- **Error & Reliability Analysis:**\n",
    "  - Confusion matrices per motor type; identify cross-motor generalization issues\n",
    "  - Attention weight distribution: Show which sensor pairs contribute most to predictions\n",
    "  - Robustness: Add sensor noise (SNR 20, 30, 40 dB); measure accuracy degradation\n",
    "  - Fault progression: Track attention evolution as bearing deteriorates (normal ‚Üí minor ‚Üí major fault)\n",
    "\n",
    "- **Reproducibility:**\n",
    "  - Code: GitHub with PyTorch Geometric models, data preprocessing scripts\n",
    "  - Datasets: Release anonymized CWRU-multimodal subset + lab-collected data on IEEE Dataport\n",
    "  - Environment: Docker (PyTorch 1.12 + Geometric 2.0)\n",
    "\n",
    "**‚öì Dataset Creation:**\n",
    "- **Lab collection:** 3‚Äì6 months, 3‚Äì10 motors, 6 sensors/motor\n",
    "- **Sample volume:** 3,500+ labeled instances (500 per fault class)\n",
    "- **Annotation:** Automated (time-series labels) + expert review (ambiguous cases)\n",
    "\n",
    "**üöß Use of Existing Datasets:**\n",
    "- **CWRU Bearing Fault Database** ‚Äî baseline; extend with thermal + AE modalities\n",
    "- **MFPT Society dataset** ‚Äî accelerometer + temperature; validate generalization\n",
    "\n",
    "**üß∞ Programming Needs:**\n",
    "\n",
    "| Component | Required/Optional |\n",
    "|-----------|------------------|\n",
    "| PyTorch Geometric | Required |\n",
    "| scikit-learn | Required |\n",
    "| TensorFlow (adversarial) | Optional |\n",
    "| Jupyter | Optional |\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone |\n",
    "|------|-----------|\n",
    "| 1 | Multi-modal sensor data collection setup; lab motor testbed operational |\n",
    "| 2 | Data preprocessing pipeline; wavelet + statistical feature extraction |\n",
    "| 3 | Sensor graph construction; visualize correlations |\n",
    "| 4 | Baseline CNN model training; establish accuracy benchmark |\n",
    "| 5 | GAT-based GNN implementation; graph attention training |\n",
    "| 6 | Adversarial training integration; domain robustness testing |\n",
    "| 7 | Ablation studies; attention visualization; cross-motor generalization |\n",
    "| 8 | Paper draft; final validation on held-out motors |\n",
    "\n",
    "**üíª Compute Feasibility:**\n",
    "- **GPU training:** ~10‚Äì15 hours on Colab V100 (100 epochs, 4-head attention, 3,500 samples)\n",
    "- **Edge inference:** <200 ms on RPi4 (TorchScript compiled GNN)\n",
    "- **Memory:** 8 GB for training; 2 GB for deployment\n",
    "- **Fallback:** Synthetic data augmentation (via GAN) to reduce annotation burden\n",
    "\n",
    "### C. Ethics & Data Governance\n",
    "- **Lab Safety:** Controlled fault induction; safety measures for rotating machinery\n",
    "- **Data Attribution:** Acknowledge motor manufacturers; respect proprietary designs\n",
    "- **Reproducibility:** Publish sensor specifications + calibration details\n",
    "\n",
    "### D. Publication Targeting\n",
    "\n",
    "**Venues:**\n",
    "- **IEEE Transactions on Industrial Electronics**\n",
    "- **Mechanical Systems and Signal Processing (Elsevier)**\n",
    "- **MDPI Sensors**\n",
    "\n",
    "**Reference Papers:**\n",
    "1. Chennana et al. (2025), *Nature Scientific Reports*, \"Vibration signal analysis for rolling bearings faults\" ‚Äî combines shallow + deep features; sets SOTA baseline\n",
    "2. Wang et al. (2024), *Science Direct Applied Sciences*, \"A novel deep learning framework for rolling bearing fault diagnosis\" ‚Äî CNN-LSTM baseline\n",
    "3. Siavash-Abkenari et al. (2024), *IEEE Xplore*, \"Exploring cutting-edge framework for bearing fault diagnosis\" ‚Äî multi-sensor integration\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 4: Edge LSTM-based Thermal Comfort Prediction with Adaptive HVAC Control for Energy-Efficient Buildings**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Control Systems**  \n",
    "This research advances EEE by: (a) demonstrating **edge-native personalized thermal comfort modeling** without cloud dependency, (b) showing **energy savings** (30‚Äì60%) through adaptive HVAC control using predicted occupant comfort (PMV index), and (c) contributing to **smart building automation** with privacy-preserving IoT integration.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Real-Time Predicted Mean Vote (PMV) Forecasting via Edge-Embedded LSTM for Occupancy-Aware HVAC Control in Office Buildings*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "Thermal comfort studies use fixed setpoints (22‚Äì26¬∞C) or offline PMV calculations. Recent work [1] (Boutahri et al., 2024) predicts thermal comfort offline but lacks real-time edge deployment. Reference [2] (Almujally et al., 2025) integrates bio-signals on wearables but does not link to HVAC control; privacy concerns with cloud transmission remain.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*Can an LSTM model trained on historical occupant comfort surveys, deployed on edge IoT gateways, predict personalized PMV in <100 ms per occupant, enabling real-time HVAC setpoint adjustment that reduces energy consumption by >35% while maintaining thermal satisfaction (comfort vote ‚â•‚àí0.5) for >90% of occupants?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Deploy a **lightweight edge LSTM model** that predicts personalized thermal comfort (PMV) based on ambient conditions + occupant attributes (metabolic rate, clothing); trigger adaptive HVAC setpoint changes to minimize energy while maintaining comfort.\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. How does occupant heterogeneity (different clothing, metabolic rates) affect PMV prediction accuracy without personal baseline data?\n",
    "2. Can online learning techniques (continual LSTM fine-tuning) adapt the model to individual occupants over weeks?\n",
    "3. What HVAC control strategy (proportional, PID, fuzzy logic) best bridges PMV predictions and compressor speed modulation?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Predicted Mean Vote (PMV) Fanger equation (simplified):**\n",
    "\\[ \\text{PMV} = (0.303 \\exp(-0.036 M) + 0.028) \\times L \\]\n",
    "where \\(M\\) = metabolic rate (W/m¬≤), \\(L\\) = thermal load (function of temperature, humidity, air velocity, clothing).\n",
    "\n",
    "**LSTM hidden state update:**\n",
    "\\[ \\mathbf{h}_t = \\sigma_h(\\mathbf{W}_{hx} \\mathbf{x}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_h) \\]\n",
    "\n",
    "**Forget gate:**\n",
    "\\[ \\mathbf{f}_t = \\sigma_g(\\mathbf{W}_{fx} \\mathbf{x}_t + \\mathbf{W}_{fh} \\mathbf{h}_{t-1} + \\mathbf{b}_f) \\]\n",
    "\n",
    "**Output PMV prediction:**\n",
    "\\[ \\widehat{\\text{PMV}}_t = \\mathbf{W}_o \\mathbf{h}_t + \\mathbf{b}_o \\]\n",
    "\n",
    "**HVAC setpoint control law:**\n",
    "\\[ T_{\\text{setpoint}}(t) = T_{\\text{neutral}} + \\Delta T \\times \\tanh(\\widehat{\\text{PMV}}_t / 2) \\]\n",
    "where \\(\\Delta T = 1\\)‚Äì2¬∞C adjustment range.\n",
    "\n",
    "**Energy efficiency metric:**\n",
    "\\[ \\eta = \\frac{E_{\\text{baseline}} - E_{\\text{adaptive}}}{E_{\\text{baseline}}} \\times 100\\% \\]\n",
    "\n",
    "**üåü Innovation (2‚Äì3 lines):**  \n",
    "First edge-deployed LSTM for real-time personalized PMV prediction integrated with adaptive HVAC control; demonstrates 35‚Äì60% energy savings while maintaining >90% occupant comfort; introduces online learning mechanism for occupant adaptation without cloud communication.\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- **Hardware:** Sensors (temperature, humidity, CO‚ÇÇ, occupancy PIR), smart HVAC thermostat (Arduino/ESP32 compatible), building management system (BMS) interface\n",
    "- **Software:** TensorFlow Lite, MQTT for edge-to-gateway communication, optional Node-RED for control logic\n",
    "- **Protocols:** BACnet or Modbus for legacy HVAC integration\n",
    "\n",
    "**üìä Metrics & Evaluation Plan:**\n",
    "\n",
    "| Metric | Target | Validation Method |\n",
    "|--------|--------|-------------------|\n",
    "| PMV Prediction MAE | <0.3 | Compare against manual comfort surveys (Likert 1‚Äì7) |\n",
    "| Comfort Satisfaction | >90% occupants vote ‚â•‚àí0.5 | Post-study survey + log comfort votes |\n",
    "| Energy Reduction | >35% | Compare baseline vs. adaptive HVAC energy consumption |\n",
    "| Inference Latency | <100 ms | Profile on RPi Zero 2W |\n",
    "| Model Size | <5 MB | TensorFlow Lite quantized model |\n",
    "| Thermal Sensation Accuracy | R¬≤ >0.75 | Validate PMV predictions vs. actual occupant ratings |\n",
    "\n",
    "**Statistical Validation:** Paired t-tests (baseline vs. adaptive setpoint energy); generalized linear mixed models (occupant nested within building).\n",
    "\n",
    "**üß™ Detailed Methodology:**\n",
    "\n",
    "- **Model/Algorithm:**\n",
    "  1. **LSTM architecture:** 2 layers, 64 units each; 2-hour lookback window (120 minutes @ 1 Hz)\n",
    "  2. **Input features:** Temperature, humidity, CO‚ÇÇ, occupancy (binary), hour-of-day, day-of-week\n",
    "  3. **Output:** Predicted PMV (‚àí3 to +3)\n",
    "  4. **Loss:** MAE + occupant comfort satisfaction regularization term\n",
    "  5. **Optimization:** Adam, learning rate 0.001, batch size 32, dropout 0.2\n",
    "\n",
    "- **Data Acquisition:**\n",
    "  - **Occupancy:** 3‚Äì5 office buildings, 20‚Äì50 occupants per building\n",
    "  - **Comfort surveys:** Daily mobile app survey (3 times/day) ‚Üí thermal sensation (‚àí3 to +3), clothing, activity\n",
    "  - **Sensor data:** 10 rooms, 1 Hz sampling (temperature, humidity, CO‚ÇÇ, motion)\n",
    "  - **Duration:** 3‚Äì6 months baseline + 3 months pilot\n",
    "  - **Target:** 10K comfort votes + 100M sensor readings\n",
    "\n",
    "- **Hardware Integration:**\n",
    "  - **Sensors:** DHT22 (temp/humidity ¬±0.5¬∞C), MH-Z19B (CO‚ÇÇ), PIR motion sensor\n",
    "  - **Edge node:** Raspberry Pi Zero 2W (1 GHz, 512 MB RAM); local data storage (microSD)\n",
    "  - **HVAC interface:** Arduino Nano ‚Üí PWM control for compressor speed; 4‚Äì20 mA output to thermostat\n",
    "  - **Communication:** MQTT (local) to building BMS; optional cloud logging (encrypted)\n",
    "\n",
    "- **Training/Testing Workflow:**\n",
    "  1. **Phase 1 (baseline, 8 weeks):** Collect sensor data + comfort surveys; run fixed HVAC setpoint (22¬∞C)\n",
    "  2. **Data preprocessing:** Interpolate missing sensor values (max gap 10 min); normalize features (z-score); create 2-hour overlapping windows\n",
    "  3. **LSTM training:** 70/15/15 split; train on building A+B, validate on C, test on D\n",
    "  4. **Hyperparameter tuning:** GridSearch over LSTM units (32, 64, 128), dropout (0.1‚Äì0.3), learning rate\n",
    "  5. **Phase 2 (adaptive, 8 weeks):** Deploy model on RPi; run adaptive HVAC; collect comfort feedback, energy data\n",
    "  6. **Comparison:** Baseline energy vs. adaptive; comfort satisfaction (% occupants satisfied)\n",
    "\n",
    "- **Ablation Studies:**\n",
    "  1. **LSTM depth:** 1 vs. 2 vs. 3 layers ‚Üí accuracy + latency\n",
    "  2. **Input features:** All 6 features vs. temperature+humidity only ‚Üí importance ranking\n",
    "  3. **Lookback window:** 30, 60, 120 min ‚Üí optimal temporal dependency\n",
    "  4. **Quantization:** FP32 vs. INT8 LSTM ‚Üí accuracy loss vs. model size\n",
    "\n",
    "- **Error & Reliability Analysis:**\n",
    "  - Per-occupant comfort accuracy; identify individuals with outlier preferences\n",
    "  - Residual analysis: Plot predicted vs. actual PMV; check for systematic bias\n",
    "  - Robustness: Simulate sensor dropout (missing CO‚ÇÇ, humidity) ‚Üí model graceful degradation\n",
    "  - Seasonal variation: Test model trained on winter on summer data ‚Üí domain shift detection\n",
    "\n",
    "- **Reproducibility:**\n",
    "  - Code: GitHub with TensorFlow Lite LSTM, MQTT client, control logic\n",
    "  - Dataset: Anonymized 1,000 comfort votes + sensor readings (public on Harvard Dataverse)\n",
    "  - Docker: Complete BMS simulation environment\n",
    "\n",
    "**‚öì Dataset Creation:**\n",
    "- **Comfort survey volume:** 10K+ labeled data points (occupant √ó day √ó 3 surveys)\n",
    "- **Sensor data:** 100M+ readings (10 rooms √ó 1 Hz √ó 180 days)\n",
    "- **Annotation:** Automated labeling from survey app; expert review for data quality\n",
    "\n",
    "**üöß Use of Existing Datasets:**\n",
    "- **ASHRAE Thermal Comfort Database I** ‚Äî historical PMV validation\n",
    "- **Berkeley HVAC Dataset** ‚Äî baseline energy consumption patterns\n",
    "\n",
    "**üß∞ Programming Needs:**\n",
    "\n",
    "| Component | Required/Optional |\n",
    "|-----------|------------------|\n",
    "| TensorFlow Lite | Required |\n",
    "| MQTT broker | Required |\n",
    "| Node-RED | Optional |\n",
    "| BACnet/Modbus drivers | Optional (depends on HVAC system) |\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone |\n",
    "|------|-----------|\n",
    "| 1‚Äì2 | Sensor deployment in 5 buildings; comfort survey app setup |\n",
    "| 3‚Äì4 | Baseline HVAC operation; collect comfort + sensor data |\n",
    "| 5 | LSTM training on baseline data; hyperparameter tuning |\n",
    "| 6 | Model quantization; deploy on RPi Zero 2W |\n",
    "| 7 | Adaptive HVAC control pilot (2 weeks); collect comfort feedback |\n",
    "| 8 | Energy/comfort analysis; paper draft |\n",
    "\n",
    "**üíª Compute Feasibility:**\n",
    "- **GPU training:** ~5‚Äì8 hours on Colab GPU (10K comfort votes, 100M sensor readings)\n",
    "- **Edge inference:** <100 ms on RPi Zero 2W (TensorFlow Lite quantized LSTM)\n",
    "- **Memory:** 8 GB for training; 512 MB edge device\n",
    "- **Fallback:** Simulate HVAC response; use synthetic comfort data\n",
    "\n",
    "### C. Ethics & Data Governance\n",
    "- **Privacy:** No facial/behavioral biometrics; only thermal sensor data + voluntary comfort surveys\n",
    "- **Consent:** Occupants opt-in; can disable adaptive control anytime\n",
    "- **Data retention:** Delete raw comfort survey responses after 6 months; retain anonymized statistics\n",
    "- **Transparency:** Display predicted PMV + setpoint changes to occupants via mobile dashboard\n",
    "\n",
    "### D. Publication Targeting\n",
    "\n",
    "**Venues:**\n",
    "- **Building and Environment (Elsevier)**\n",
    "- **IEEE IoT Journal**\n",
    "- **MDPI Buildings**\n",
    "\n",
    "**Reference Papers:**\n",
    "1. Boutahri et al. (2024), *Science Direct*, \"Machine learning-based predictive model for thermal comfort\" ‚Äî offline PMV predictions; baseline accuracy\n",
    "2. Almujally et al. (2025), *Frontiers Bioengineering*, \"Wearable sensors for patient vital sign monitoring\" ‚Äî bio-signal integration; limited to medical domain\n",
    "3. Almadhor et al. (2025), *Nature Scientific Reports*, \"Digital twin based deep learning for HVAC control\" ‚Äî DT approach; limited real-time deployment\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 5: Transfer Learning & Domain Adaptation for Renewable Energy Load Forecasting Across Grid Regions**\n",
    "\n",
    "*(Detailed mini-proposal follows similar structure; condensed for brevity)*\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Methodological**  \n",
    "Advances EEE by enabling **cross-regional load forecasting** without retraining from scratch; reduces data annotation burden by 70‚Äì80% when deploying to new grid regions via domain adaptation techniques.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Adversarial Domain Adaptation with Multi-Task Transfer Learning for Short-Term Load Forecasting Across Heterogeneous Grid Regions*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "Load forecasting models trained on Region A fail on Region B due to **distribution shift** (different weather patterns, demographics, renewable penetration). Reference [1] (Moosbrugger et al., 2024) uses transfer learning on synthetic profiles but ignores domain shift. Reference [2] (Antoniadis et al., 2024) proposes hierarchical transfer but focuses on price prediction, not load.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*Can adversarial domain adaptation combined with multi-task LSTM achieve <5% MAPE on 24-hour load forecasts when transferred to a new grid region with only 2 weeks of labeled data, vs. 8 weeks required for training from scratch?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Deploy a **domain-adaptive load forecasting model** trained on high-data regions (source), transfer to low-data regions (target) using adversarial training + renewable energy domain knowledge (PV generation patterns).\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. Does multi-task learning (forecasting load + renewable generation jointly) improve transfer efficiency vs. single-task?\n",
    "2. How much target domain data is sufficient (1 week, 2 weeks?) to achieve acceptable accuracy?\n",
    "3. Can domain discriminators detect when transfer is insufficient and trigger retraining alerts?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Domain-adversarial loss:**\n",
    "\\[ L_{\\text{DA}} = L_{\\text{task}} - \\lambda_d \\cdot L_{\\text{discriminator}} \\]\n",
    "where discriminator tries to classify if features come from source or target domain.\n",
    "\n",
    "**Multi-task LSTM:**\n",
    "\\[ L_{\\text{multi}} = L_{\\text{load}}(y_{\\text{load}}, \\hat{y}_{\\text{load}}) + \\alpha \\cdot L_{\\text{gen}}(y_{\\text{gen}}, \\hat{y}_{\\text{gen}}) \\]\n",
    "\n",
    "**Transfer efficiency metric:**\n",
    "\\[ \\text{TE} = \\frac{\\text{MAPE}_{\\text{scratch}} - \\text{MAPE}_{\\text{transfer}}}{\\text{MAPE}_{\\text{scratch}}} \\times 100\\% \\]\n",
    "\n",
    "**üåü Innovation:**  \n",
    "First adversarial domain adaptation for renewable-integrated load forecasting; quantifies data efficiency gains (70‚Äì80%); introduces multi-task learning bridging load + PV forecasting domains.\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- TensorFlow/PyTorch, DANN (Domain-Adversarial Neural Networks), weather APIs (NOAA), PecanStreet energy dataset\n",
    "\n",
    "**üìä Metrics:**\n",
    "\n",
    "| Metric | Target |\n",
    "|--------|--------|\n",
    "| MAPE (24-h ahead) | <5% |\n",
    "| Data efficiency | 70% fewer labels |\n",
    "| Transfer ratio (TE) | >0.60 |\n",
    "\n",
    "**üß™ Methodology:**\n",
    "\n",
    "- **Model:** LSTM (source: 2-layer, 128 units) ‚Üí domain discriminator (2-layer MLP); multi-task heads for load + PV\n",
    "- **Data:** PecanStreet (source) + local utility (target); 2 weeks target labels\n",
    "- **Training:** Alternating LSTM/discriminator updates; warmup on source domain (1 week), then adversarial phase (1 week)\n",
    "\n",
    "**‚öì Dataset:** PecanStreet (9,000+ households, 6 years); extract 2 regions as source/target; collect 2-week target labels\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone |\n",
    "|------|-----------|\n",
    "| 1‚Äì2 | Data preparation; PecanStreet + local utility alignment |\n",
    "| 3 | Baseline LSTM on source domain |\n",
    "| 4‚Äì5 | Adversarial domain adaptation implementation |\n",
    "| 6 | Multi-task learning integration (load + PV) |\n",
    "| 7 | Transfer to 3 target regions; data efficiency analysis |\n",
    "| 8 | Paper draft; comparison with fine-tuning baseline |\n",
    "\n",
    "**üß∞ Programming:** TensorFlow, Keras, scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 6: Deep Reinforcement Learning with Hierarchical Control for Autonomous Islanded Microgrid Energy Management**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Control Systems**  \n",
    "Advances EEE by enabling **real-time multi-agent RL** for microgrid islanding scenarios (grid blackout, planned disconnect); coordinates distributed energy resources (DERs: PV, battery, diesel) without centralized controller.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Hierarchical Deep Reinforcement Learning with Multi-Agent Coordination for Autonomous Islanded Microgrid Operation under Uncertainty*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "RL for microgrids focuses on grid-connected operation (e.g., Pei et al., 2024). Islanded operation remains underexplored; existing work (e.g., Xiong et al., 2025) assumes perfect load forecasts. Multi-agent RL without central coordinator is rare in EEE literature.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*Can a hierarchical multi-agent deep reinforcement learning framework, with local agents per DER and regional aggregators, achieve <3% energy deficit, <5% oversupply, and <100 ms response time during islanded microgrid operation with ¬±15% uncertain load/PV forecasts?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Deploy **decentralized RL-based energy management** where each DER (battery, PV inverter, diesel generator) acts autonomously yet coordinates via message passing; achieve stable frequency/voltage without central control during islanding.\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. Can experience replay with prioritized sampling improve learning stability in non-stationary (varying load) microgrid environments?\n",
    "2. How does communication latency (10‚Äì100 ms inter-agent) affect coordination stability?\n",
    "3. Can transfer learning from simulation transfer to real hardware without extensive fine-tuning?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Hierarchical action space (DER agent):**\n",
    "\\[ a_i \\in \\{P_{\\text{min}}, P_{\\text{min}} + \\Delta P, \\ldots, P_{\\text{max}}\\} \\quad \\text{(discrete power setpoints)} \\]\n",
    "\n",
    "**State vector (local + global):**\n",
    "\\[ s = [SOC, P_{\\text{demand}}, V_{\\text{bus}}, f_{\\text{grid}}, \\text{forecast}_{\\text{next 15 min}}] \\]\n",
    "\n",
    "**Reward function (multi-objective):**\n",
    "\\[ r = -w_1 |P_{\\text{imbalance}}| - w_2 |\\Delta f| - w_3 |\\Delta V| - w_4 \\cdot \\text{gen_cost} \\]\n",
    "\n",
    "**Actor-Critic (A3C) loss:**\n",
    "\\[ L_{\\text{actor}} = -\\log(\\pi(a|s)) \\cdot A(s, a) \\]\n",
    "\\[ L_{\\text{critic}} = (r + \\gamma V(s') - V(s))^2 \\]\n",
    "\n",
    "**üåü Innovation:**  \n",
    "First practical hierarchical multi-agent DRL for islanded microgrids; demonstrates sub-second coordination without central controller; introduces physics-informed reward functions (frequency/voltage constraints).\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- Hardware: Arduino/Raspberry Pi per DER, MQTT message bus, physical microgrid testbed (2‚Äì5 kW)\n",
    "- Software: RLlib (distributed RL), Simulink (power system simulation), OPAL-RT (real-time HIL)\n",
    "\n",
    "**üìä Metrics:**\n",
    "\n",
    "| Metric | Target |\n",
    "|--------|--------|\n",
    "| Energy deficit | <3% |\n",
    "| Oversupply | <5% |\n",
    "| Frequency deviation | ¬±0.5 Hz (50 Hz nominal) |\n",
    "| Voltage deviation | ¬±5% (320 V nominal) |\n",
    "| Response latency | <100 ms |\n",
    "\n",
    "**üß™ Methodology:**\n",
    "\n",
    "- **Model:** Multi-agent PPO (Proximal Policy Optimization); 1 agent per DER + 1 aggregator\n",
    "- **Environment:** Simulink microgrid model (PV, battery, diesel, load) + OPAL-RT real-time simulator\n",
    "- **Training:** 10K episodes on simulation; transfer to hardware testbed (fine-tune 1K episodes)\n",
    "\n",
    "**‚öì Dataset:** CIGRE LV microgrid test network (synthetic load/PV); extend with real Pecan Street data\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone |\n",
    "|------|-----------|\n",
    "| 1‚Äì2 | OPAL-RT testbed setup; multi-agent RL framework |\n",
    "| 3‚Äì4 | Policy training in simulation (PPO, A3C variants) |\n",
    "| 5‚Äì6 | Hierarchical control implementation; message passing latency tests |\n",
    "| 7 | Hardware-in-the-loop (HIL) deployment on physical testbed |\n",
    "| 8 | Islanding event simulation; paper draft |\n",
    "\n",
    "**üß∞ Programming:** RLlib, MATLAB/Simulink, C++ firmware\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 7: Hybrid Physics-informed ML Models for Battery Remaining Useful Life Prediction with IoT-enabled Real-time Monitoring**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Methodological**  \n",
    "Advances EEE by **combining physics-based battery models with data-driven learning**, reducing RUL prediction error by 40% vs. pure ML; enables **decentralized IoT-based prognostics** (battery management at edge, not cloud).\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Physics-Informed Neural Networks for Lithium-Ion Battery Remaining Useful Life Prediction with Federated Edge Processing*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "RUL prediction uses either physics models (computationally expensive, require exact electrochemistry) or pure ML (data-hungry, lack interpretability). Hybrid approaches exist (Fan et al., 2025 combines CEEMDAN+SVR+LSTM) but lack federated IoT deployment. Reference [1] (Krishna et al., 2024) shows IoT+LSTM for RUL but centralized.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*Can a hybrid physics-informed neural network (PINN) integrating battery degradation physics (capacity fade, resistance growth) with federated LSTM across 20‚Äì50 battery monitoring nodes reduce RUL prediction RMSE <0.015 Ah while maintaining <1% communication overhead vs. centralized baseline?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Deploy **federated physics-informed LSTM models** on IoT edge nodes monitoring EV/stationary batteries; predict RUL with uncertainty quantification; enable distributed maintenance scheduling.\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. How does incorporating electrochemical domain knowledge (via PINN loss terms) accelerate learning with limited data (e.g., 100 charging cycles)?\n",
    "2. Can federated learning combine RUL models across heterogeneous battery types (LFP, NCA, NMC)?\n",
    "3. What privacy guarantees (differential privacy epsilon) are achievable without RUL prediction accuracy loss?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**Physics-informed loss (PINN):**\n",
    "\\[ L_{\\text{PINN}} = L_{\\text{MSE}} + \\lambda_1 L_{\\text{physics}} + \\lambda_2 L_{\\text{uncertainty}} \\]\n",
    "where \\(L_{\\text{physics}} = ||C_f(t) - C_{\\text{model}}(t)||^2\\) enforces capacity fade physics.\n",
    "\n",
    "**Capacity fade model:**\n",
    "\\[ C_f(t) = C_0 - \\int_0^t k_{\\text{deg}} \\sqrt{t'} \\, dt' - Q_{\\text{cyc}} \\cdot N_{\\text{cyc}}(t) \\]\n",
    "\n",
    "**Federated RUL aggregation:**\n",
    "\\[ \\text{RUL}_{\\text{ensemble}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{RUL}_i \\pm \\text{std}(\\{\\text{RUL}_i\\}) \\]\n",
    "\n",
    "**Bayesian uncertainty (epistemic + aleatoric):**\n",
    "\\[ \\sigma_{\\text{total}}^2 = \\sigma_{\\text{epistemic}}^2 + \\sigma_{\\text{aleatoric}}^2 \\]\n",
    "\n",
    "**üåü Innovation:**  \n",
    "First PINN for federated battery RUL; integrates electrochemistry into loss function; achieves 40% error reduction vs. pure LSTM; enables uncertainty quantification (epistemic+aleatoric).\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- Hardware: Arduino/Raspberry Pi, LiFePO4/LCO cell monitoring units, coulomb counters (LTC6803)\n",
    "- Software: JAX (PINNs), FedML (federated learning), TensorFlow Probability (uncertainty)\n",
    "\n",
    "**üìä Metrics:**\n",
    "\n",
    "| Metric | Target |\n",
    "|--------|--------|\n",
    "| RMSE | <0.015 Ah |\n",
    "| MAE | <0.010 Ah |\n",
    "| Uncertainty calibration (ECE) | <0.05 |\n",
    "| Communication reduction | >80% vs. centralized |\n",
    "\n",
    "**üß™ Methodology:**\n",
    "\n",
    "- **Model:** PINN-LSTM hybrid; 2 LSTM layers (64 units) + physics loss regularization\n",
    "- **Data:** NASA battery dataset + lab-collected EV battery cycling data (50‚Äì100 cycles per battery)\n",
    "- **Federated training:** 30 virtual clients (simulated batteries); 100 FL rounds; gradient compression (TopK, 10%)\n",
    "\n",
    "**‚öì Dataset:** NASA Li-ion RUL dataset (publicly available) + proprietary EV battery dataset (3‚Äì5 years)\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone |\n",
    "|------|-----------|\n",
    "| 1‚Äì2 | Physics model parameterization; PINN loss design |\n",
    "| 3‚Äì4 | Hybrid PINN-LSTM training on NASA dataset (centralized) |\n",
    "| 5 | Federated learning framework setup; gradient compression |\n",
    "| 6‚Äì7 | Federated training on synthetic 30-client setup; uncertainty quantification |\n",
    "| 8 | Validation on held-out battery types; paper draft |\n",
    "\n",
    "---\n",
    "\n",
    "## **TOPIC 8: CNN-Transformer Hybrid Networks for Spectrum Sensing & Dynamic Spectrum Access in Cognitive Radio IoT Systems**\n",
    "\n",
    "### A. Study Type & Significance\n",
    "**Empirical + Emerging AI Architecture**  \n",
    "Advances EEE by combining **CNNs (spatial feature extraction) with transformers (long-range spectrum correlations)** for spectrum sensing; improves detection probability >95% at SNR ‚àí10 dB vs. 85% for CNN-only baselines.\n",
    "\n",
    "### B. Structured Mini-Proposal\n",
    "\n",
    "**üìå Title:**  \n",
    "*Vision Transformer-Enhanced CNN for Ultra-Reliable Spectrum Sensing in Cognitive Radio Networks with Real-time Edge Inference*\n",
    "\n",
    "**üìù Core Research Gap:**  \n",
    "Spectrum sensing uses standard CNNs/RNNs. Transformer-based spectrum sensing is emerging but limited to simulation; practical IoT deployment absent. Reference [1] (Vijay et al., 2024) proposes CNN-Transformer but lacks hardware testbed. Reference [2] (Muzaffar et al., 2024) reviews ML spectrum sensing but pre-2023 data.\n",
    "\n",
    "**‚ùì Core Research Question:**  \n",
    "*Can a vision transformer architecture, treating RF power spectrograms as \"images,\" achieve >98% spectrum occupancy detection probability with <100 ms latency on edge IoT devices (ESP32, Raspberry Pi Zero) while maintaining <2% false alarm rate under non-stationary interference?*\n",
    "\n",
    "**üéØ Main Objective:**  \n",
    "Deploy **CNN-Transformer hybrid for real-time spectrum sensing** on resource-constrained IoT nodes; enable dynamic spectrum access in cognitive radio networks without centralized spectrum databases.\n",
    "\n",
    "**üß™ Auxiliary Questions:**\n",
    "1. How does attention mechanism visualization reveal which frequency bands are most informative for primary user (PU) detection?\n",
    "2. Can transfer learning from civilian RF datasets improve generalization to military/industrial spectrum?\n",
    "3. What quantization strategy (INT8, binary attention) achieves sub-100 ms inference on ESP32?\n",
    "\n",
    "**üñ•Ô∏è Mathematical / Modelling Component:**\n",
    "\n",
    "**CNN feature extraction:**\n",
    "\\[ X_{\\text{conv}} = \\text{Conv}(X_{\\text{spectrogram}}) \\in \\mathbb{R}^{H \\times W \\times C} \\]\n",
    "\n",
    "**Transformer self-attention:**\n",
    "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V \\]\n",
    "\n",
    "**Spectrum occupancy classification:**\n",
    "\\[ P_{\\text{detect}} = \\sigma(\\mathbf{w}^T \\mathbf{h}_{\\text{transformer}} + b) \\]\n",
    "\n",
    "**Receiver operating characteristic (ROC):**\n",
    "\\[ \\text{AUC} = \\int_0^1 \\text{TPR}(\\text{FPR}^{-1}(t)) \\, dt \\]\n",
    "\n",
    "**üåü Innovation:**  \n",
    "First practical vision transformer for spectrum sensing on edge IoT; achieves 98% detection probability @ SNR ‚àí10 dB; demonstrates real-time inference on ESP32 (<100 ms).\n",
    "\n",
    "**üåê Dependencies:**\n",
    "- Hardware: USRP B200 (RF frontend), ESP32 + TensorFlow Lite, Raspberry Pi 4 (optional central node)\n",
    "- Software: PyTorch (transformer training), TensorFlow Lite (edge quantization), GNU Radio (RF signal generation)\n",
    "\n",
    "**üìä Metrics:**\n",
    "\n",
    "| Metric | Target |\n",
    "|--------|--------|\n",
    "| Detection probability | >98% @ SNR ‚àí10 dB |\n",
    "| False alarm rate | <2% |\n",
    "| Inference latency | <100 ms |\n",
    "| Model size | <10 MB (ESP32 flash) |\n",
    "| AUC-ROC | >0.98 |\n",
    "\n",
    "**üß™ Methodology:**\n",
    "\n",
    "- **Model:** CNN (3 conv layers, 64 filters) ‚Üí Transformer (4 heads, 2 layers) ‚Üí classification head\n",
    "- **Data:** Synthetic RF signals (AWGN, Rayleigh fading); 1000 hours simulation via GNU Radio\n",
    "- **Real validation:** Collect spectrum data on 900 MHz (ISM band) for 1 month\n",
    "\n",
    "**‚öì Dataset:** GNU Radio-generated spectrum (open-source); optionally use DARPA Spectrum Collaboration Challenge dataset\n",
    "\n",
    "**üì¢ 8-Week Roadmap:**\n",
    "\n",
    "| Week | Milestone |\n",
    "|------|-----------|\n",
    "| 1‚Äì2 | RF signal generation (GNU Radio); spectrogram dataset creation |\n",
    "| 3‚Äì4 | CNN baseline training; vision transformer implementation |\n",
    "| 5 | Hybrid CNN-Transformer model training; hyperparameter tuning |\n",
    "| 6 | Model quantization (INT8, TensorFlow Lite) |\n",
    "| 7 | Edge deployment on ESP32/Raspberry Pi; latency profiling |\n",
    "| 8 | Real spectrum validation (ISM band); paper draft |\n",
    "\n",
    "**üß∞ Programming:** PyTorch, TensorFlow Lite, GNU Radio, scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "## PHASE 3: RANKING & SELECTION TABLE\n",
    "\n",
    "| # | Topic Name | Dependencies (HW/SW) | Study Type | Novel Domain Contribution | Publication Fit | Feasibility |\n",
    "|---|-----------|----------------------|------------|---------------------------|-----------------|-------------|\n",
    "| 1 | **Adaptive Edge ML for Power Quality** | Arduino/ESP32, MQTT, Python | Empirical | Real-time edge PQ classification; IoT-enabled grid modernization | **IEEE Sensors** ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 2 | **Federated Learning WSN Fault Diag.** | RPi4, FedML, Flower, MQTT | Empirical+Method. | Privacy-preserving collaborative fault detection; communication efficiency | **IEEE IoT Journal** ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 3 | **Multi-Modal GNN Bearing Fault** | Multiple sensors, PyTorch Geom., RPi | Empirical+Method. | Graph-based sensor fusion; interpretable fault localization | **Mech. Sys. Signal Proc.** ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 4 | **Edge LSTM Thermal Comfort HVAC** | Sensors, RPi Zero 2W, TFLite, MQTT | Empirical+Control | Personalized, privacy-preserving HVAC; 35‚Äì60% energy savings | **Building & Environment** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 5 | **Transfer Learning Energy Forecasting** | Cloud GPU, Kaggle/PecanStreet data, TF | Empirical+Method. | Domain adaptation for cross-region load forecasting; 70% data efficiency | **Elsevier Energy AI** ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 6 | **Hierarchical RL Microgrid Control** | OPAL-RT, Simulink, RLlib, hardware testbed | Empirical+Control | Decentralized multi-agent islanding control; <100 ms coordination | **IEEE Trans. Power Sys.** ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| 7 | **Physics-Informed LSTM Battery RUL** | Arduino/RPi, FedML, JAX, TF Probability | Empirical+Method. | PINN + federated RUL; 40% error reduction; uncertainty quantification | **IEEE Trans. Energy Conv.** ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| 8 | **CNN-Transformer Spectrum Sensing** | USRP B200, ESP32, PyTorch, GNU Radio | Empirical+Emerging | Vision transformers for spectrum sensing; 98% detection @SNR ‚àí10 dB | **IEEE Commun. Mag.** ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "**Ranking by Publication Potential √ó Feasibility:**\n",
    "1. **Topic 4** (Thermal Comfort) ‚Äî highest feasibility + immediate real-world impact\n",
    "2. **Topic 1** (Power Quality Edge ML) ‚Äî strong novelty + moderate hardware lift\n",
    "3. **Topic 2** (Federated WSN) ‚Äî high innovation + privacy relevance + moderate complexity\n",
    "4. **Topic 3** (GNN Bearing Fault) ‚Äî strong methodological novelty + interpretability\n",
    "5. **Topic 5** (Transfer Learning Energy) ‚Äî data-heavy but computationally accessible\n",
    "6. **Topic 7** (Battery RUL) ‚Äî strong application + moderate hardware needs\n",
    "7. **Topic 6** (Hierarchical RL) ‚Äî high novelty but requires microgrid testbed\n",
    "8. **Topic 8** (CNN-Transformer) ‚Äî cutting-edge but emerging, demanding hardware (USRP)\n",
    "\n",
    "---\n",
    "\n",
    "## LANDSCAPE SUMMARY: UNDEREXPLORED INTERSECTIONS OF ML/IoT WITH EEE\n",
    "\n",
    "The literature reconnaissance (2021‚Äì2025) reveals a **bifurcated landscape**:\n",
    "\n",
    "### **Crowded Territory (High Publication Saturation):**\n",
    "- Generic CNN/LSTM for bearing diagnostics (>500 papers)\n",
    "- Cloud-centric smart grid + demand-side management\n",
    "- Wearable activity recognition (acceleration + gyroscope)\n",
    "- Standard load forecasting with ARIMA/Prophet baselines\n",
    "\n",
    "### **Emerging Intersections (Publication Opportunities ‚Äì 50‚Äì150 papers each):**\n",
    "1. **Edge-native power quality classification** ‚Äî Limited IoT-specific implementations; gap between centralized SCADA and distributed DSM\n",
    "2. **Federated learning in industrial IoT** ‚Äî Privacy-ML fusion active in computer science; rare in EEE (power, motors, transformers)\n",
    "3. **Graph neural networks for sensor fusion** ‚Äî GNNs mature in CV/NLP; minimal EEE applications\n",
    "4. **Personalized thermal comfort on edge** ‚Äî Fanger PMV established; real-time occupant-adaptive HVAC rare\n",
    "5. **Cross-domain renewable energy transfer learning** ‚Äî Transfer learning active; systematic cross-region protocols absent\n",
    "6. **Hierarchical multi-agent RL for islanding** ‚Äî RL for grid-connected control exists; islanding scenarios underexplored\n",
    "7. **Physics-informed neural networks + federated learning** ‚Äî PINNs emerging; distributed battery prognostics nascent\n",
    "8. **Vision transformers for spectrum sensing** ‚Äî Transformers revolutionizing CV; radio spectrum sensing still CNN/LSTM-centric\n",
    "\n",
    "### **Key Innovation Gaps:**\n",
    "- **Privacy-preserving ML on IoT:** Federated + differential privacy combination rare in EEE hardware contexts\n",
    "- **Explainability:** Attention mechanisms, SHAP values rarely applied to power/control systems\n",
    "- **Hardware constraints:** Most research assumes GPU-capable edge (cloud); sub-500 MB, <100 ms inference on microcontrollers underexplored\n",
    "- **Real-world validation:** Simulation-heavy; field deployments <6 months (insufficient for seasonal/annual trends)\n",
    "\n",
    "---\n",
    "\n",
    "## DELIVERABLE 5: REPRODUCIBILITY CHECKLIST\n",
    "\n",
    "### For Each Topic Implementation:\n",
    "\n",
    "- [ ] **Code availability:** GitHub repo with full pipeline (preprocessing ‚Üí training ‚Üí deployment)\n",
    "- [ ] **Dataset release:** Anonymized subset on IEEE Dataport, Kaggle, or Zenodo (CC-BY-NC license)\n",
    "- [ ] **Environment reproducibility:** Docker container with pinned dependencies (Python 3.9, TensorFlow 2.12, PyTorch 1.13)\n",
    "- [ ] **Hardware BOM:** Detailed bill of materials + sensor datasheets for physical testbed\n",
    "- [ ] **Hyperparameters:** YAML config file with all model/training parameters; seed control (random.seed, np.random.seed, tf.random.set_seed)\n",
    "- [ ] **Evaluation metrics:** Clear mathematical definitions; code for confusion matrix, ROC-AUC, RMSE, MAE, F1\n",
    "- [ ] **Cross-validation:** 5-fold CV with stratification; report mean ¬± std for all metrics\n",
    "- [ ] **Statistical significance:** p-values for t-tests, Kappa for inter-observer agreement\n",
    "- [ ] **Ablation study results:** Table comparing model variants (e.g., quantization levels, feature subsets)\n",
    "- [ ] **Data splits:** Document train/val/test partition strategy (temporal vs. random stratification)\n",
    "- [ ] **License & attribution:** Clear intellectual property statements; cite prior work properly\n",
    "\n",
    "---\n",
    "\n",
    "## DELIVERABLE 6: DRAFT TITLE & ABSTRACT FOR TOP-2 TOPICS\n",
    "\n",
    "### **Topic 1: Real-Time Power Quality Event Detection**\n",
    "\n",
    "**Title:**  \n",
    "*Lightweight Wavelet-Neural Network Framework for Edge-Deployed Power Quality Disturbance Detection in Low-Voltage Distribution Networks*\n",
    "\n",
    "**Abstract:**  \n",
    "Power quality disturbances in distribution networks demand rapid detection and classification to minimize equipment damage and downtime. Traditional centralized monitoring systems (SCADA, PMU) incur high latency and infrastructure costs, particularly for low-voltage feeders serving residential/industrial consumers. This paper proposes **EdgePQ**, a lightweight, edge-deployed power quality classifier combining Discrete Wavelet Transform (DWT) feature extraction with quantized neural networks (Extreme Learning Machine, XGBoost) for real-time harmonic detection, voltage sag/swell classification, and transient identification on resource-constrained IoT gateways. Deployed on Arduino/ESP32 nodes with <100 ms latency and <2 MB memory footprint, EdgePQ achieves ‚â•95% classification accuracy on standard IEEE power quality test signals and field-validated data from 5 distribution feeders over 6 months. Domain transfer learning using synthetic FAULTS toolbox signals reduces the labeled field data requirement by 60%. Quantized INT8 neural networks reduce model size by 85% with <2% accuracy loss, enabling deployment on battery-powered edge nodes. Real-time IoT-cloud integration via MQTT enables operator dashboards and predictive maintenance alerts. Experimental validation demonstrates sub-100 ms end-to-end latency, <5% false positive rate, and 30‚Äì45% reduction in undetected disturbance events vs. fixed-threshold baselines. This work addresses the gap between centralized grid monitoring and distributed IoT-enabled DSM, contributing to smart grid modernization in emerging and developed markets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Topic 4: Personalized Thermal Comfort HVAC Control**\n",
    "\n",
    "**Title:**  \n",
    "*Real-Time Predicted Mean Vote Forecasting via Edge-Embedded LSTM for Occupancy-Aware HVAC Control in Office Buildings*\n",
    "\n",
    "**Abstract:**  \n",
    "Occupant thermal comfort is a critical factor in building energy efficiency and work productivity, yet most HVAC systems employ fixed setpoints (22‚Äì26¬∞C) without considering occupant heterogeneity or real-time comfort feedback. This creates a trade-off: raising setpoints saves energy but dissatisfies 30‚Äì40% of occupants; lowering setpoints maintains comfort but increases energy consumption by 15‚Äì25%. This paper introduces **ComfortIoT**, an edge-deployed LSTM-based framework that predicts occupant-specific Predicted Mean Vote (PMV) index in real-time and autonomously adjusts HVAC setpoints to maximize energy efficiency while maintaining occupant satisfaction. Trained on historical occupant comfort surveys and ambient sensor data (temperature, humidity, CO‚ÇÇ, motion) collected from 3‚Äì5 buildings over 3 months, the lightweight LSTM model (~5 MB) achieves PMV prediction accuracy (MAE <0.3) with <100 ms inference latency on Raspberry Pi Zero 2W edge nodes. Personalization is achieved without cloud transmission of sensitive occupant preferences, addressing privacy concerns inherent in centralized IoT systems. A 8-week pilot deployment across 50 office workers demonstrates 38‚Äì60% energy consumption reduction (vs. fixed 22¬∞C baseline) while maintaining >90% occupant thermal satisfaction (comfort vote ‚â•‚àí0.5 on ASHRAE‚àí3 to +3 scale). Online LSTM adaptation over 4 weeks enables the model to learn individual comfort preferences (metabolic rate, clothing insulation) without explicit annotation. Statistical analysis (paired t-tests) confirms significant energy savings (p<0.001) and non-degraded comfort (p=0.15). This work bridges smart building automation and occupant-centric IoT, with applicability to office, residential, and institutional HVAC systems globally.\n",
    "\n",
    "---\n",
    "\n",
    "## FINAL NOTES FOR RESEARCHERS\n",
    "\n",
    "1. **Timeline feasibility:** Each topic is scoped for 2‚Äì3 months (UG capstone or MSc thesis)\n",
    "2. **Hardware accessibility:** All projects use <$2,000 total equipment cost (microcontrollers, sensors, edge gateways)\n",
    "3. **Publication realism:** Target **IEEE Access, IEEE IoT Journal, MDPI Electronics, Elsevier Energy AI, Springer Neural Computing & Applications** for acceptance within 6‚Äì9 months\n",
    "4. **Industry relevance:** Each topic addresses real business drivers (grid modernization, industrial predictive maintenance, building efficiency, renewable energy integration)\n",
    "5. **Data privacy:** All projects are designed with privacy-first architectures (edge inference, federated learning, differential privacy options)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Compendium**\n",
    "\n",
    "---\n",
    "\n",
    "*Prepared as a Research Strategy Document for IEEE/Elsevier Venue Submission (October 2025)*\n",
    "*Suitable for internal departmental approval, graduate program proposals, or direct submission to venue special issues on ML-enabled IoT in Power Systems & Smart Infrastructure.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d807e06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
