{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6756bfa7",
   "metadata": {},
   "source": [
    "# Class 5: ML Basics — Linear Regression (Calibration & Curve Fitting)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Linear regression is the lab scale of ML: you put in an input, it gives a prediction with a straight-line relationship. In EE/IoT land, you’ll use it to calibrate sensors, correct drift, or estimate quantities that are hard to measure directly.\n",
    "\n",
    "---\n",
    "\n",
    "### Lecture Notes (short, teachable)\n",
    "\n",
    "* **Goal**: learn a mapping ( y \\approx \\beta_0 + \\beta_1 x ) (or multi-feature ( \\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta} )).\n",
    "* **Train / test split**: measure generalization honestly.\n",
    "* **Metrics**: MAE (average absolute error), MSE (squared), (R^2) (explained variance).\n",
    "* **Residuals**: ( r_i = y_i - \\hat{y}_i ); plot them — they confess model sins (nonlinearity, heteroscedasticity, outliers).\n",
    "* **Common mistakes**: fitting on test data, leaking future info, ignoring units/scales/outliers.\n",
    "\n",
    "EE hooks:\n",
    "\n",
    "* Calibrate thermistor: temperature → resistance (near-linear in a small range).\n",
    "* Battery health: cycle count/features → capacity drop (piecewise/approx linear).\n",
    "* Sensor cross-calibration: raw reading → true reference value.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Notebook (Colab-style cells)\n",
    "\n",
    "### 0) Imports + synthetic calibration dataset\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Synthetic \"sensor calibration\": true_y = 2.5 + 0.8*x + noise\n",
    "n = 120\n",
    "x = np.linspace(0, 50, n)                              # e.g., temperature in °C\n",
    "noise = np.random.normal(0, 1.2, size=n)               # measurement noise\n",
    "y_true = 2.5 + 0.8 * x                                 # ideal mapping\n",
    "y = y_true + noise                                     # observed reading to be predicted\n",
    "\n",
    "df = pd.DataFrame({\"temp_C\": x, \"reading\": y, \"ideal\": y_true})\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 1) Train/test split and model fit\n",
    "\n",
    "```python\n",
    "X = df[[\"temp_C\"]].values\n",
    "y = df[\"reading\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Intercept (β0):\", model.intercept_)\n",
    "print(\"Slope (β1):\", model.coef_[0])\n",
    "```\n",
    "\n",
    "### 2) Evaluate — metrics + predicted line\n",
    "\n",
    "```python\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "r2  = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"MAE={mae:.3f}, MSE={mse:.3f}, R^2={r2:.3f}\")\n",
    "\n",
    "# Visualize fit against all data\n",
    "x_grid = np.linspace(df[\"temp_C\"].min(), df[\"temp_C\"].max(), 200).reshape(-1, 1)\n",
    "y_line = model.predict(x_grid)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(df[\"temp_C\"], df[\"reading\"], s=20, label=\"data\", alpha=0.7)\n",
    "plt.plot(x_grid, y_line, label=\"fitted line\", linewidth=2)\n",
    "plt.title(\"Linear Regression Fit (Sensor Calibration)\")\n",
    "plt.xlabel(\"Temperature (°C)\")\n",
    "plt.ylabel(\"Reading (units)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 3) Residual analysis (plot + quick checks)\n",
    "\n",
    "```python\n",
    "# Residuals on the test set\n",
    "resid = y_test - y_pred_test\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred_test, resid, s=25)\n",
    "plt.axhline(0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Residuals vs Predicted (Test)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Residual mean (should be ~0):\", np.mean(resid))\n",
    "print(\"Residual std:\", np.std(resid))\n",
    "```\n",
    "\n",
    "### 4) Multi-feature quick demo (add a small linear drift feature)\n",
    "\n",
    "```python\n",
    "# Add a second feature that could represent another sensor or derived feature\n",
    "df[\"drift\"] = 0.02 * df[\"temp_C\"]  # tiny drift correlated with temp\n",
    "\n",
    "X2 = df[[\"temp_C\", \"drift\"]].values\n",
    "y2 = df[\"reading\"].values\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.25, random_state=0)\n",
    "\n",
    "model2 = LinearRegression().fit(X2_train, y2_train)\n",
    "y2_pred = model2.predict(X2_test)\n",
    "\n",
    "print(\"Coefs (β):\", model2.coef_, \"| Intercept:\", model2.intercept_)\n",
    "print(\"R^2 (2 features):\", r2_score(y2_test, y2_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Classworks (5) — Skeleton Code (students fill the blanks)\n",
    "\n",
    "### Classwork 1: Fit a line and report metrics\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 1: SIMPLE LINEAR REGRESSION\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Build a synthetic dataset: y = a + b*x + noise (choose a, b).\n",
    "# 2) Split train/test.\n",
    "# 3) Fit LinearRegression, compute MAE, MSE, R^2 on test.\n",
    "# 4) Plot data + fitted line.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# x = np.linspace(???, ???, ???).reshape(-1, 1)\n",
    "# noise = np.random.normal(0, ???, size=x.shape[0])\n",
    "# y = ??? + ??? * x.flatten() + noise\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(???, ???, test_size=0.2, random_state=0)\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(???, ???)\n",
    "\n",
    "# y_pred = model.predict(???)\n",
    "# mae = mean_absolute_error(???, ???)\n",
    "# mse = mean_squared_error(???, ???)\n",
    "# r2  = r2_score(???, ???)\n",
    "# print(f\"MAE={mae:.3f}, MSE={mse:.3f}, R^2={r2:.3f}\")\n",
    "\n",
    "# # plot\n",
    "# x_grid = np.linspace(x.min(), x.max(), 200).reshape(-1, 1)\n",
    "# y_line = model.predict(x_grid)\n",
    "# plt.figure()\n",
    "# plt.scatter(x, y, s=20, label=\"data\", alpha=0.7)\n",
    "# plt.plot(x_grid, y_line, label=\"fit\", linewidth=2)\n",
    "# plt.title(\"Simple Linear Regression\")\n",
    "# plt.xlabel(\"x\")\n",
    "# plt.ylabel(\"y\")\n",
    "# plt.grid(True); plt.legend(); plt.show()\n",
    "```\n",
    "\n",
    "### Classwork 2: Residuals check + assumptions\n",
    "\n",
    "```python\n",
    "# ==========================================\n",
    "# CLASSWORK 2: RESIDUAL DIAGNOSTICS\n",
    "# ==========================================\n",
    "# Task:\n",
    "# 1) Reuse your model from CW1.\n",
    "# 2) Compute residuals on TEST set: r = y_test - y_pred.\n",
    "# 3) Plot residuals vs predicted.\n",
    "# 4) Print mean and std of residuals.\n",
    "\n",
    "# resid = ??? - ???\n",
    "# import numpy as np, matplotlib.pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.scatter(???, resid, s=25)\n",
    "# plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "# plt.title(\"Residuals vs Predicted\")\n",
    "# plt.xlabel(\"Predicted\"); plt.ylabel(\"Residual\")\n",
    "# plt.grid(True); plt.show()\n",
    "\n",
    "# print(\"Residual mean:\", np.mean(???))\n",
    "# print(\"Residual std:\", np.std(???))\n",
    "```\n",
    "\n",
    "### Classwork 3: Add an irrelevant feature (sanity test)\n",
    "\n",
    "```python\n",
    "# =======================================================\n",
    "# CLASSWORK 3: ADD AN IRRELEVANT FEATURE (NOISE FEATURE)\n",
    "# =======================================================\n",
    "# Task:\n",
    "# 1) Generate a random feature z (same length as x), uncorrelated with y.\n",
    "# 2) Fit LinearRegression on [x, z].\n",
    "# 3) Compare R^2 with 1-feature model. Does it improve meaningfully?\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# z = np.random.randn(x.shape[0]).reshape(-1, 1)\n",
    "# X2 = np.hstack([x, z])\n",
    "# X2_train, X2_test, y_train, y_test = train_test_split(???, ???, test_size=0.2, random_state=0)\n",
    "\n",
    "# model1 = LinearRegression().fit(X_train, y_train)   # from CW1\n",
    "# model2 = LinearRegression().fit(X2_train, y_train)\n",
    "\n",
    "# r2_1 = r2_score(y_test, model1.predict(X_test))\n",
    "# r2_2 = r2_score(y_test, model2.predict(X2_test))\n",
    "# print(\"R^2 (1 feature):\", r2_1, \" | R^2 (x + noise z):\", r2_2)\n",
    "```\n",
    "\n",
    "### Classwork 4: Mild nonlinearity — where linear fails\n",
    "\n",
    "```python\n",
    "# =======================================================\n",
    "# CLASSWORK 4: MILD NONLINEARITY CHECK\n",
    "# =======================================================\n",
    "# Task:\n",
    "# 1) Create data: y = 1.0 + 0.2*x + 0.02*x^2 + noise (slight curve).\n",
    "# 2) Fit linear model on x only.\n",
    "# 3) Plot residuals vs x: do you see structure (curve)?\n",
    "# 4) Optional: add x^2 as a feature and show improvement in R^2.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(1)\n",
    "# x2 = np.linspace(???, ???, ???).reshape(-1, 1)\n",
    "# noise = np.random.normal(0, 0.5, size=x2.shape[0])\n",
    "# y2 = 1.0 + 0.2*x2.flatten() + 0.02*(x2.flatten()**2) + noise\n",
    "\n",
    "# # Linear (wrong) model\n",
    "# X_lin = x2\n",
    "# model_lin = LinearRegression().fit(X_lin, y2)\n",
    "# y2_pred_lin = model_lin.predict(X_lin)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(x2, y2 - y2_pred_lin, s=20)\n",
    "# plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "# plt.title(\"Residuals vs x (nonlinearity visible?)\")\n",
    "# plt.xlabel(\"x\")\n",
    "# plt.ylabel(\"residual\")\n",
    "# plt.grid(True); plt.show()\n",
    "\n",
    "# # Optional polynomial fix (manual feature)\n",
    "# X_poly = np.hstack([x2, x2**2])\n",
    "# model_poly = LinearRegression().fit(X_poly, y2)\n",
    "# print(\"R^2 linear:\", r2_score(y2, y2_pred_lin))\n",
    "# print(\"R^2 poly  :\", r2_score(y2, model_poly.predict(X_poly)))\n",
    "```\n",
    "\n",
    "### Classwork 5: Mini calibration case (CSV)\n",
    "\n",
    "```python\n",
    "# =======================================================\n",
    "# CLASSWORK 5: MINI CALIBRATION FROM CSV\n",
    "# =======================================================\n",
    "# Task:\n",
    "# 1) Create or load 'calib.csv' with columns raw_reading, true_value (at least 50 rows).\n",
    "# 2) Fit LinearRegression to map raw_reading -> true_value.\n",
    "# 3) Report MAE, MSE, R^2 on test split.\n",
    "# 4) Plot: scatter(raw_reading, true_value) + fitted line.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df = pd.read_csv(\"calib.csv\")  # or generate synthetic then save\n",
    "# X = df[[\"raw_reading\"]].values\n",
    "# y = df[\"true_value\"].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(???, ???, test_size=0.25, random_state=0)\n",
    "# model = LinearRegression().fit(???, ???)\n",
    "# y_pred = model.predict(???)\n",
    "\n",
    "# print(\"MAE:\", ???(???, ???))\n",
    "# print(\"MSE:\", ???(???, ???))\n",
    "# print(\"R^2:\", ???(???, ???))\n",
    "\n",
    "# # Plot\n",
    "# x_grid = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "# y_line = model.predict(x_grid)\n",
    "# plt.figure()\n",
    "# plt.scatter(X, y, s=20, alpha=0.7, label=\"data\")\n",
    "# plt.plot(x_grid, y_line, \"r\", label=\"fit\")\n",
    "# plt.xlabel(\"raw_reading\")\n",
    "# plt.ylabel(\"true_value\")\n",
    "# plt.title(\"Calibration Fit\")\n",
    "# plt.grid(True); plt.legend(); plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Wrap-up / Homework Challenge\n",
    "\n",
    "* **Drift-aware calibration**: Add a second feature (e.g., ambient temperature) to your CSV and compare 1-feature vs 2-feature models via (R^2) and MAE. Write 3 lines interpreting whether the extra feature is genuinely useful or just noise.\n",
    "* Bonus: save a **model card** (Markdown cell or text file) summarizing data source, splits, metrics, known limitations, and next steps (e.g., try polynomial features or regularization later).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
